{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Открытие гугл диска"
      ],
      "metadata": {
        "id": "da4NGJXFep8u"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "03oGkBwgehRl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3b244eab-99f4-4367-8ddb-0462e51c68e0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import os.path\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import random\n",
        "from torchvision import transforms\n",
        "import math\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import time"
      ],
      "metadata": {
        "id": "PzzsQ21besk8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Дата лоадер"
      ],
      "metadata": {
        "id": "9PtfeaoQetpL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_pathes(base_folder ):\n",
        "    paths = []\n",
        "    for i in range(11):\n",
        "        folder_name = f'{i:02d}'\n",
        "        image_folder = os.path.join(base_folder, folder_name, 'image_0')\n",
        "        movements_file = os.path.join(base_folder, folder_name, f'{folder_name}.txt')\n",
        "        paths.append((image_folder,movements_file))\n",
        "    return paths\n",
        "\n",
        "def get_data(batch_size, base_folder = '/content/drive/MyDrive'):\n",
        "    paths = get_pathes(base_folder)\n",
        "\n",
        "    # Выбор папок для тренировочных, валидационных и тестовых данных\n",
        "    validation_folder = paths[-2]  # Предпоследняя папка\n",
        "    test_folder = paths[-1]        # Последняя папка\n",
        "    train_folders = paths[:-2]     # Все остальные папки\n",
        "\n",
        "    # Чтение движений из файла\n",
        "    def create_movements(movements_file):\n",
        "        movements = []\n",
        "        with open(movements_file, 'r') as f:\n",
        "            for line in f:\n",
        "                data = list(map(float, line.strip().split()))\n",
        "                movements.append(data)\n",
        "        return movements\n",
        "\n",
        "    # movements = create_movements(movements_file)\n",
        "\n",
        "    # Преобразование движений\n",
        "    # def transform_mov(movements):\n",
        "    #     new_movements = []\n",
        "    #     for matrix in movements:\n",
        "    #         new_matrix = [\n",
        "    #             matrix[0:4],\n",
        "    #             matrix[4:8],\n",
        "    #             matrix[8:12],\n",
        "    #             [0, 0, 0, 1]\n",
        "    #         ]\n",
        "    #         new_matrix = torch.tensor(new_matrix, dtype=torch.float32)\n",
        "    #         new_movements.append(new_matrix)\n",
        "\n",
        "    #     res_movements = []\n",
        "    #     for i in range(len(new_movements) - 1):\n",
        "    #         T1_inv = torch.linalg.inv(new_movements[i])\n",
        "    #         T_rel = torch.matmul(new_movements[i + 1], T1_inv)\n",
        "    #         res_matrix = T_rel[:3]\n",
        "    #         res_matrix = torch.cat([res_matrix[0], res_matrix[1], res_matrix[2]])\n",
        "    #         res_movements.append(res_matrix)\n",
        "\n",
        "\n",
        "    #     return res_movements\n",
        "\n",
        "    # def transform_mov(movements):\n",
        "    #     res_movements = []\n",
        "    #     for i in range(len(movements)-1):\n",
        "    #         m1 = torch.tensor(movements[i])\n",
        "    #         m2 = torch.tensor(movements[i+1])\n",
        "    #         res = m2-m1\n",
        "    #         res_movements.append(res)\n",
        "    #     return res_movements\n",
        "    step = 1\n",
        "\n",
        "    def transform_mov(movements):\n",
        "        res_movements = []\n",
        "        # for i in range(len(movements)-1):\n",
        "        for i in range(len(movements)-step):\n",
        "            m1 = torch.tensor(movements[i])\n",
        "            m2 = torch.tensor(movements[i+step])\n",
        "            res = m2-m1\n",
        "            distance = math.sqrt(pow(res[3],2) + pow(res[7],2) + pow(res[11],2))\n",
        "            res_movements.append([distance ])\n",
        "        return res_movements\n",
        "\n",
        "\n",
        "\n",
        "    # movements = transform_mov(movements)\n",
        "\n",
        "    # Оптимизированный класс Dataset\n",
        "    class ImageDataset(Dataset):\n",
        "        def __init__(self, image_folder, movements, transform=None ):\n",
        "            # Получаем список всех файлов .png и сортируем их\n",
        "            all_image_files = sorted([os.path.join(image_folder, file) for file in os.listdir(image_folder) if file.endswith('.png')])\n",
        "            self.image_files = all_image_files\n",
        "            self.movements = movements\n",
        "            self.transform = transform\n",
        "\n",
        "        def __len__(self):\n",
        "            # Возвращаем минимальную длину между движениями и изображениями - 1\n",
        "            return min(len(self.movements), len(self.image_files) - 1)\n",
        "\n",
        "\n",
        "        def __getitem__(self, idx):\n",
        "            image1_path = self.image_files[idx]\n",
        "            # image2_path = self.image_files[idx + 1]\n",
        "            image2_path = self.image_files[idx + step]\n",
        "\n",
        "            image1 = Image.open(image1_path).convert('L')\n",
        "            image2 = Image.open(image2_path).convert('L')\n",
        "\n",
        "            if self.transform:\n",
        "                image1 = self.transform(image1)\n",
        "                image2 = self.transform(image2)\n",
        "\n",
        "            movement = self.movements[idx].clone().detach().float()if isinstance(self.movements[idx], torch.Tensor) else torch.tensor(self.movements[idx], dtype=torch.float32)\n",
        "            return (image1, image2), movement\n",
        "\n",
        "\n",
        "    # Трансформации для уменьшения использования памяти\n",
        "    transform = transforms.Compose([\n",
        "        transforms.Grayscale(),\n",
        "        # transforms.Resize((376,1240)),\n",
        "        transforms.CenterCrop((370,1226)),\n",
        "        # transforms.Resize((180,320)),\n",
        "        transforms.ToTensor()\n",
        "    ])\n",
        "\n",
        "    def load_dataset(folders):\n",
        "        images, movements = [], []\n",
        "        for image_folder, movements_file in folders:\n",
        "            mov = create_movements(movements_file)\n",
        "            mov = transform_mov(mov)\n",
        "            movements.append(mov)\n",
        "            images.append(image_folder)\n",
        "        return images, movements\n",
        "\n",
        "    train_images, train_movements = load_dataset(train_folders)\n",
        "    train_dataset = []\n",
        "    for index,image_folder in enumerate(train_images):\n",
        "        dataset = ImageDataset(image_folder, train_movements[index], transform=transform)\n",
        "        train_dataset.append(dataset)\n",
        "    train_dataset = torch.utils.data.ConcatDataset(train_dataset)\n",
        "\n",
        "    # Загрузка валидационных данных\n",
        "    val_images, val_movements = load_dataset([validation_folder])\n",
        "    val_dataset = ImageDataset(val_images[0], val_movements[0], transform=transform)\n",
        "\n",
        "\n",
        "   # Загрузка тестовых данных\n",
        "    test_images, test_movements = load_dataset([test_folder])\n",
        "    test_dataset = ImageDataset(test_images[0], test_movements[0], transform=transform)\n",
        "\n",
        "    # DataLoaders\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle = True, num_workers=4, pin_memory=True)\n",
        "    validation_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle = False, num_workers=4, pin_memory=True)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle = False, num_workers=4, pin_memory=True)\n",
        "\n",
        "    return train_loader, validation_loader, test_loader\n"
      ],
      "metadata": {
        "id": "HFQdWvw3evPX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "EVALUATION"
      ],
      "metadata": {
        "id": "e5KxNCpLewz-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def set_all_seeds(seed):\n",
        "    os.environ[\"PL_GLOBAL_SEED\"] = str(seed)\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "def compute_accuracy(model, data_loader, device):\n",
        "    model.eval()\n",
        "    total_loss = 0.0\n",
        "    num_batches = 0\n",
        "    correct_predictions = 0   # Счетчик корректных предсказаний\n",
        "    total_samples = 0         # Общее количество элементов\n",
        "\n",
        "    criterion = torch.nn.MSELoss()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, ((feature1, feature2),targets) in enumerate(data_loader):\n",
        "            feature1 = feature1.to(device)\n",
        "            feature2 = feature2.to(device)\n",
        "            targets = targets.to(device)\n",
        "\n",
        "            predicted_y = model(feature1, feature2)\n",
        "\n",
        "            # Вычисление ошибки\n",
        "            loss = criterion(predicted_y, targets)\n",
        "            total_loss += loss.item()\n",
        "            num_batches += 1\n",
        "\n",
        "            # Проверка критерия качества\n",
        "            relative_difference = torch.abs((predicted_y - targets))  # Расчет абсолютного отклонения\n",
        "            valid_predictions = torch.all(relative_difference <= 0.1, dim=1)    # Условие: отклонение <= 10 см\n",
        "            correct_predictions += torch.sum(valid_predictions).item()\n",
        "            total_samples = total_samples + targets.size(0)\n",
        "\n",
        "\n",
        "    avg_loss = total_loss / num_batches             # Расчет средней ошибки\n",
        "    accuracy = correct_predictions / total_samples  # Расчет доли верных предсказаний\n",
        "    return avg_loss, accuracy"
      ],
      "metadata": {
        "id": "JHbXDiteeyNI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model"
      ],
      "metadata": {
        "id": "jUfPohmye0RP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Network(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        self.features = nn.Sequential(\n",
        "            nn.Conv2d(1, 32, kernel_size = (7,7), stride = (2,2), padding=(3, 3)),# 185, 613\n",
        "            nn.BatchNorm2d(32),\n",
        "            nn.ReLU(inplace = True),\n",
        "\n",
        "            nn.Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),# 185, 613\n",
        "            nn.BatchNorm2d(32),\n",
        "            nn.ReLU(inplace = True),\n",
        "\n",
        "            nn.Conv2d(32, 64, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2)),#93, 307\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(inplace = True),\n",
        "\n",
        "            nn.Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),# 93, 307\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(inplace = True),\n",
        "\n",
        "            nn.Conv2d(64, 128, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2)),#47, 154\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.ReLU(inplace = True),\n",
        "\n",
        "            nn.Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),#47, 154\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.ReLU(inplace = True),\n",
        "\n",
        "            nn.Conv2d(128,256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1)),# 24, 77\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.ReLU(inplace = True),\n",
        "\n",
        "            nn.Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),# 24, 77\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.ReLU(inplace = True),\n",
        "\n",
        "            nn.Conv2d(256,512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1)),# 12, 39\n",
        "            nn.BatchNorm2d(512),\n",
        "            nn.ReLU(inplace = True),\n",
        "\n",
        "            nn.Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),# 12, 39\n",
        "            nn.BatchNorm2d(512),\n",
        "            nn.ReLU(inplace = True),\n",
        "\n",
        "            nn.Conv2d(512, 512, kernel_size=3, stride=2, padding=1),  #  6 × 20\n",
        "            nn.BatchNorm2d(512),\n",
        "            nn.ReLU(inplace=True),\n",
        "\n",
        "            nn.Conv2d(512, 512, kernel_size=3, stride=2, padding=1),  #  3 × 10\n",
        "            nn.BatchNorm2d(512),\n",
        "            nn.ReLU(inplace=True),\n",
        "\n",
        "            # nn.AdaptiveAvgPool2d((5, 5))  # Глобальный пуллинг к фиксированному размеру\n",
        "        )\n",
        "        self.lstm1 = nn.LSTMCell(input_size = 512 * 3 * 10, hidden_size=128)\n",
        "        self.lstm2 = nn.LSTMCell(input_size = 512 * 3 * 10, hidden_size=128)\n",
        "\n",
        "        self.network = nn.Sequential(\n",
        "            nn.Linear(128, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self,image1,image2):\n",
        "        out_features1 = self.features(image1)\n",
        "        out_features2 = self.features(image2)\n",
        "        out_features1 = torch.flatten(out_features1, 1)\n",
        "        out_features2 = torch.flatten(out_features2, 1)\n",
        "        batch_size = out_features1.size(0)\n",
        "\n",
        "        # Инициализируем скрытые состояния и состояния ячеек для lstm1\n",
        "        h_t1 = torch.zeros(batch_size, self.lstm1.hidden_size, device=out_features1.device)\n",
        "        c_t1 = torch.zeros(batch_size, self.lstm1.hidden_size, device=out_features1.device)\n",
        "\n",
        "        # Пропускаем признаки первой картинки через lstm1 (один временной шаг)\n",
        "        h_t1_next, c_t1_next = self.lstm1(out_features1, (h_t1, c_t1))\n",
        "        h_t2_next, c_t2_next = self.lstm2(out_features2, (h_t1_next, c_t1_next))\n",
        "        out = self.network(h_t2_next) # [batch_size, 1]\n",
        "        return out\n",
        "\n",
        "\n",
        "    # def forward(self,image1,image2):\n",
        "    #     images = torch.cat((image1,image2),dim = 1)\n",
        "\n",
        "    #     out_features = self.features(images)\n",
        "    #     out_features = torch.flatten(out_features, 1)\n",
        "    #     batch_size = out_features.size(0)\n",
        "\n",
        "    #     # Инициализируем скрытые состояния и состояния ячеек для lstm1\n",
        "    #     h_t1 = torch.zeros(batch_size, self.lstm1.hidden_size, device=out_features.device)\n",
        "    #     c_t1 = torch.zeros(batch_size, self.lstm1.hidden_size, device=out_features.device)\n",
        "\n",
        "    #     # Пропускаем признаки первой картинки через lstm1 (один временной шаг)\n",
        "    #     h_t1_next, c_t1_next = self.lstm1(out_features, (h_t1, c_t1))\n",
        "\n",
        "    #     # Пропускаем признаки второй картинки через lstm2 (один временной шаг)\n",
        "    #     h_t2_next, c_t2_next = self.lstm2(h_t1_next, (h_t1_next, c_t1_next))\n",
        "\n",
        "    #     out = self.network(h_t2_next) # [batch_size, 1]\n",
        "    #     return out"
      ],
      "metadata": {
        "id": "eo_J1uEGezmQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train"
      ],
      "metadata": {
        "id": "3dIlvrYye3JE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, num_epochs, last_epoch, train_loader,\n",
        "                validation_loader, test_loader,optimizer, device,scheduler,\n",
        "          checkpoint_dict):\n",
        "    start_time = time.time()\n",
        "    minibatch_loss_list, train_loss_list, valid_loss_list = [] ,[], []\n",
        "    criterion = torch.nn.MSELoss()\n",
        "\n",
        "    for epoch in range(last_epoch, num_epochs):\n",
        "        model.train()\n",
        "        for batch_idx,((feature1,feature2),targets) in enumerate(train_loader):\n",
        "            feature1 = feature1.to(device)\n",
        "            feature2 = feature2.to(device)\n",
        "            targets = targets.to(device)\n",
        "\n",
        "            predicted_y = model(feature1,feature2)\n",
        "            loss = criterion(predicted_y, targets)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            minibatch_loss_list.append(loss.item())\n",
        "            if batch_idx %100 == 0:\n",
        "                print(f'Epoch: {epoch+1:03d}/{num_epochs:03d} '\n",
        "                      f'| Batch {batch_idx:04d}/{len(train_loader):04d} '\n",
        "                      f'| Loss: {loss:.4f}'\n",
        "                      f'| Learning rate: {optimizer.param_groups[0][\"lr\"]}')\n",
        "        # scheduler.step()\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            avg_loss_train, accuracy_train = compute_accuracy(model, train_loader, device)\n",
        "            avg_loss_val, accuracy_val = compute_accuracy(model, validation_loader, device)\n",
        "            # scheduler.step( avg_loss_val)\n",
        "            print(f'Epoch: {epoch+1:03d}/{num_epochs:03d} '\n",
        "                  f'| Train AVG LOSS: {avg_loss_train: .4f} | Accuracy_train: {accuracy_train: .4f} \\n'\n",
        "                  f'| Validation AVG LOSS: {avg_loss_val: .4f} | Accuracy_val: {accuracy_val: .4f} ')\n",
        "            train_loss_list.append(avg_loss_train)\n",
        "            valid_loss_list.append(avg_loss_val)\n",
        "\n",
        "        scheduler.step()\n",
        "\n",
        "        # checkpoint = {\n",
        "        #     'model_state_dict': model.state_dict(),\n",
        "        #     'optimizer_state_dict': optimizer.state_dict()\n",
        "        # }\n",
        "\n",
        "        checkpoint_dict['state_model'] = model.state_dict()\n",
        "        checkpoint_dict['state_opt'] =  optimizer.state_dict()\n",
        "        checkpoint_dict[ 'state_scheduler'] = scheduler.state_dict()\n",
        "        checkpoint_dict['train_loss'] = avg_loss_train\n",
        "        checkpoint_dict['val_loss'] = avg_loss_val\n",
        "        checkpoint_dict['train_acc'] = accuracy_train\n",
        "        checkpoint_dict['val_acc'] =  accuracy_val\n",
        "        checkpoint_dict['EPOCHS'] = num_epochs\n",
        "        checkpoint_dict['current_epoch'] = epoch + 1\n",
        "        checkpoint_dict[ 'learning'] = optimizer.param_groups[0][\"lr\"]\n",
        "\n",
        "        path = directory_path + str(epoch + 1) + '_epoch.pth'\n",
        "        torch.save(checkpoint_dict, path)\n",
        "        # path = '/content/drive/My Drive/checkpoint_model_3.pth'\n",
        "        # torch.save(checkpoint, path)\n",
        "\n",
        "        elapsed = (time.time() - start_time)/60\n",
        "        print(f'Time elapsed: {elapsed:.2f} min')\n",
        "\n",
        "    elapsed = (time.time() - start_time)/60\n",
        "    print(f'Total Training Time: {elapsed:.2f} min')\n",
        "\n",
        "    avg_loss_test, accuracy_test = compute_accuracy(model, test_loader,device)\n",
        "    print(f'Test AVG LOSS: {avg_loss_test: .4f} | Accuracy_test: {accuracy_test: .4f}')\n",
        "\n",
        "    return minibatch_loss_list, train_loss_list, valid_loss_list\n"
      ],
      "metadata": {
        "id": "ea_QjJiZe1qQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Описание модели, оптимизатора, шедулера"
      ],
      "metadata": {
        "id": "PUhv1JcDe5hr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "txt_model_opt_shed = '''\n",
        "class Network(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        self.features = nn.Sequential(\n",
        "            nn.Conv2d(2, 32, kernel_size = (7,7), stride = (2,2), padding=(3, 3)),# 185, 613\n",
        "            nn.BatchNorm2d(32),\n",
        "            nn.ReLU(inplace = True),\n",
        "\n",
        "            nn.Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),# 185, 613\n",
        "            nn.BatchNorm2d(32),\n",
        "            nn.ReLU(inplace = True),\n",
        "\n",
        "            nn.Conv2d(32, 64, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2)),#93, 307\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(inplace = True),\n",
        "\n",
        "            nn.Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),# 93, 307\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(inplace = True),\n",
        "\n",
        "            nn.Conv2d(64, 128, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2)),#47, 154\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.ReLU(inplace = True),\n",
        "\n",
        "            nn.Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),#47, 154\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.ReLU(inplace = True),\n",
        "\n",
        "            nn.Conv2d(128,256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1)),# 24, 77\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.ReLU(inplace = True),\n",
        "\n",
        "            nn.Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),# 24, 77\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.ReLU(inplace = True),\n",
        "\n",
        "            nn.Conv2d(256,512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1)),# 12, 39\n",
        "            nn.BatchNorm2d(512),\n",
        "            nn.ReLU(inplace = True),\n",
        "\n",
        "            nn.Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),# 12, 39\n",
        "            nn.BatchNorm2d(512),\n",
        "            nn.ReLU(inplace = True),\n",
        "\n",
        "            nn.Conv2d(512, 512, kernel_size=3, stride=2, padding=1),  #  6 × 20\n",
        "            nn.BatchNorm2d(512),\n",
        "            nn.ReLU(inplace=True),\n",
        "\n",
        "            nn.Conv2d(512, 512, kernel_size=3, stride=2, padding=1),  #  3 × 10\n",
        "            nn.BatchNorm2d(512),\n",
        "            nn.ReLU(inplace=True),\n",
        "\n",
        "            # nn.AdaptiveAvgPool2d((5, 5))  # Глобальный пуллинг к фиксированному размеру\n",
        "        )\n",
        "        self.lstm1 = nn.LSTMCell(input_size = 512 * 3 * 10, hidden_size=128)\n",
        "        self.lstm2 = nn.LSTMCell(input_size = 128, hidden_size=128)\n",
        "\n",
        "        self.network = nn.Sequential(\n",
        "            nn.Linear(128, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self,image1,image2):\n",
        "        images = torch.cat((image1,image2),dim = 1)\n",
        "\n",
        "        out_features = self.features(images)\n",
        "        out_features = torch.flatten(out_features, 1)\n",
        "        batch_size = out_features.size(0)\n",
        "\n",
        "        # Инициализируем скрытые состояния и состояния ячеек для lstm1\n",
        "        h_t1 = torch.zeros(batch_size, self.lstm1.hidden_size, device=out_features.device)\n",
        "        c_t1 = torch.zeros(batch_size, self.lstm1.hidden_size, device=out_features.device)\n",
        "\n",
        "        # Пропускаем признаки первой картинки через lstm1 (один временной шаг)\n",
        "        h_t1_next, c_t1_next = self.lstm1(out_features, (h_t1, c_t1))\n",
        "\n",
        "        # Пропускаем признаки второй картинки через lstm2 (один временной шаг)\n",
        "        h_t2_next, c_t2_next = self.lstm2(h_t1_next, (h_t1_next, c_t1_next))\n",
        "\n",
        "        out = self.network(h_t2_next) # [batch_size, 1]\n",
        "        return out\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr = 0.0001)\n",
        "\n",
        "'''"
      ],
      "metadata": {
        "id": "jV_pdeV8e8wr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Создание библиотеки для хранения сохранения"
      ],
      "metadata": {
        "id": "9oZuIlHWe-Hi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "directory_path = '/content/drive/My Drive/checpoint/model_lstm2/'\n",
        "\n",
        "# Проверка существования директории\n",
        "if not os.path.exists(directory_path):\n",
        "    # Создание директории\n",
        "    os.makedirs(directory_path)\n",
        "    print(f\"Директория {directory_path} была создана.\")\n",
        "else:\n",
        "    print(f\"Директория {directory_path} уже существует.\")"
      ],
      "metadata": {
        "id": "JTfaN1Kpe_7Q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "210634d9-3be6-472d-fd7a-b2eab7bba5ba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Директория /content/drive/My Drive/checpoint/model_lstm2/ была создана.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Создание словаря для сохранения результатов"
      ],
      "metadata": {
        "id": "gbWHxfYRfByD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "checkpoint_dict = {\n",
        "    'model_description' : txt_model_opt_shed,\n",
        "    'state_model' : None,\n",
        "    'state_opt'   : None,\n",
        "    'state_scheduler' : None,\n",
        "\n",
        "    'train_loss' :None,\n",
        "    'val_loss'   : None,\n",
        "    'best_loss'  : None,\n",
        "    'train_acc'  : None,\n",
        "    'val_acc'    : None,\n",
        "\n",
        "    'EPOCHS'     : None,\n",
        "    'current_epoch' : None,\n",
        "    'learning' : None\n",
        "}"
      ],
      "metadata": {
        "id": "-Qn9c-p5fDhw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "MAIN"
      ],
      "metadata": {
        "id": "3JVl4xjxfFAu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 8\n",
        "RANDOM_SEED = 123\n",
        "NUM_EPOCHS = 20\n",
        "last_epoch = 0\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"Використовується пристрій: {DEVICE}\")"
      ],
      "metadata": {
        "id": "8rYFolahfGGa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f083cb38-6829-424e-8ed8-3d858fcd868e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Використовується пристрій: cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.optim.lr_scheduler import StepLR\n",
        "set_all_seeds(RANDOM_SEED)\n",
        "\n",
        "model = Network()\n",
        "model = model.to(DEVICE)\n",
        "# optimizer = torch.optim.Adam(model.parameters(), lr = 0.0001, weight_decay = 0.0006)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr = 0.0001)\n",
        "train_loader, validation_loader, test_loader = get_data(batch_size = BATCH_SIZE)\n",
        "# scheduler = StepLR(optimizer, step_size = 4, gamma = 0.3)\n",
        "scheduler = None\n",
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "print(f'The model has {count_parameters(model):,} trainable parameters')"
      ],
      "metadata": {
        "id": "dF29lnrbfIiZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "848194cc-0dde-4631-bb16-fddbdc954693"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The model has 25,464,289 trainable parameters\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Загрузка словаря из файла\n",
        "path = './2_epoch.pth'\n",
        "checkpoint = torch.load(path)\n",
        "if scheduler == None:\n",
        "    scheduler = StepLR(optimizer, step_size = 1, gamma = 1)\n",
        "# # Загрузка состояния модели\n",
        "# save_check = checkpoint['optimizer_state_dict']\n",
        "# save_check['lr'] = 0.0003\n",
        "model.load_state_dict( checkpoint['state_model'] )\n",
        "optimizer.load_state_dict( checkpoint['state_opt'] )\n",
        "scheduler.load_state_dict( checkpoint['state_scheduler'] )\n",
        "last_epoch = checkpoint['current_epoch']\n",
        "# # Загрузка состояния оптимизатора\n",
        "# optimizer.load_state_dict(save_check)"
      ],
      "metadata": {
        "id": "N-dZbzOxfOH4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if scheduler == None:\n",
        "    scheduler = StepLR(optimizer, step_size = 1, gamma = 1)\n",
        "minibatch_loss, train_acc, valid_acc = train(model = model,\n",
        "            num_epochs = NUM_EPOCHS,\n",
        "            last_epoch = last_epoch,\n",
        "            train_loader = train_loader,\n",
        "            validation_loader = validation_loader,\n",
        "            test_loader = test_loader,\n",
        "            optimizer = optimizer,\n",
        "            device = DEVICE,\n",
        "            scheduler = scheduler,\n",
        "            checkpoint_dict = checkpoint_dict)"
      ],
      "metadata": {
        "id": "i6CUnADlfOoX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c5d5d0d7-17ff-474b-8827-cecfa09b160a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 003/020 | Batch 0000/2550 | Loss: 0.0077| Learning rate: 0.0001\n",
            "Epoch: 003/020 | Batch 0100/2550 | Loss: 0.0100| Learning rate: 0.0001\n",
            "Epoch: 003/020 | Batch 0200/2550 | Loss: 0.0145| Learning rate: 0.0001\n",
            "Epoch: 003/020 | Batch 0300/2550 | Loss: 0.0198| Learning rate: 0.0001\n",
            "Epoch: 003/020 | Batch 0400/2550 | Loss: 0.0153| Learning rate: 0.0001\n",
            "Epoch: 003/020 | Batch 0500/2550 | Loss: 0.0145| Learning rate: 0.0001\n",
            "Epoch: 003/020 | Batch 0600/2550 | Loss: 0.0096| Learning rate: 0.0001\n",
            "Epoch: 003/020 | Batch 0700/2550 | Loss: 0.0109| Learning rate: 0.0001\n",
            "Epoch: 003/020 | Batch 0800/2550 | Loss: 0.0120| Learning rate: 0.0001\n",
            "Epoch: 003/020 | Batch 0900/2550 | Loss: 0.0091| Learning rate: 0.0001\n",
            "Epoch: 003/020 | Batch 1000/2550 | Loss: 0.0079| Learning rate: 0.0001\n",
            "Epoch: 003/020 | Batch 1100/2550 | Loss: 0.0078| Learning rate: 0.0001\n",
            "Epoch: 003/020 | Batch 1200/2550 | Loss: 0.0083| Learning rate: 0.0001\n",
            "Epoch: 003/020 | Batch 1300/2550 | Loss: 0.0130| Learning rate: 0.0001\n",
            "Epoch: 003/020 | Batch 1400/2550 | Loss: 0.0216| Learning rate: 0.0001\n",
            "Epoch: 003/020 | Batch 1500/2550 | Loss: 0.0118| Learning rate: 0.0001\n",
            "Epoch: 003/020 | Batch 1600/2550 | Loss: 0.0081| Learning rate: 0.0001\n",
            "Epoch: 003/020 | Batch 1700/2550 | Loss: 0.0090| Learning rate: 0.0001\n",
            "Epoch: 003/020 | Batch 1800/2550 | Loss: 0.0076| Learning rate: 0.0001\n",
            "Epoch: 003/020 | Batch 1900/2550 | Loss: 0.0068| Learning rate: 0.0001\n",
            "Epoch: 003/020 | Batch 2000/2550 | Loss: 0.0180| Learning rate: 0.0001\n",
            "Epoch: 003/020 | Batch 2100/2550 | Loss: 0.0024| Learning rate: 0.0001\n",
            "Epoch: 003/020 | Batch 2200/2550 | Loss: 0.0129| Learning rate: 0.0001\n",
            "Epoch: 003/020 | Batch 2300/2550 | Loss: 0.0097| Learning rate: 0.0001\n",
            "Epoch: 003/020 | Batch 2400/2550 | Loss: 0.0093| Learning rate: 0.0001\n",
            "Epoch: 003/020 | Batch 2500/2550 | Loss: 0.0067| Learning rate: 0.0001\n",
            "Epoch: 003/020 | Train AVG LOSS:  0.0092 | Accuracy_train:  0.7244 \n",
            "| Validation AVG LOSS:  0.0680 | Accuracy_val:  0.3038 \n",
            "Time elapsed: 48.88 min\n",
            "Epoch: 004/020 | Batch 0000/2550 | Loss: 0.0097| Learning rate: 0.0001\n",
            "Epoch: 004/020 | Batch 0100/2550 | Loss: 0.0152| Learning rate: 0.0001\n",
            "Epoch: 004/020 | Batch 0200/2550 | Loss: 0.0128| Learning rate: 0.0001\n",
            "Epoch: 004/020 | Batch 0300/2550 | Loss: 0.0082| Learning rate: 0.0001\n",
            "Epoch: 004/020 | Batch 0400/2550 | Loss: 0.0113| Learning rate: 0.0001\n",
            "Epoch: 004/020 | Batch 0500/2550 | Loss: 0.0116| Learning rate: 0.0001\n",
            "Epoch: 004/020 | Batch 0600/2550 | Loss: 0.0302| Learning rate: 0.0001\n",
            "Epoch: 004/020 | Batch 0700/2550 | Loss: 0.0060| Learning rate: 0.0001\n",
            "Epoch: 004/020 | Batch 0800/2550 | Loss: 0.0052| Learning rate: 0.0001\n",
            "Epoch: 004/020 | Batch 0900/2550 | Loss: 0.0053| Learning rate: 0.0001\n",
            "Epoch: 004/020 | Batch 1000/2550 | Loss: 0.0192| Learning rate: 0.0001\n",
            "Epoch: 004/020 | Batch 1100/2550 | Loss: 0.0050| Learning rate: 0.0001\n",
            "Epoch: 004/020 | Batch 1200/2550 | Loss: 0.0125| Learning rate: 0.0001\n",
            "Epoch: 004/020 | Batch 1300/2550 | Loss: 0.0119| Learning rate: 0.0001\n",
            "Epoch: 004/020 | Batch 1400/2550 | Loss: 0.0037| Learning rate: 0.0001\n",
            "Epoch: 004/020 | Batch 1500/2550 | Loss: 0.0229| Learning rate: 0.0001\n",
            "Epoch: 004/020 | Batch 1600/2550 | Loss: 0.0131| Learning rate: 0.0001\n",
            "Epoch: 004/020 | Batch 1700/2550 | Loss: 0.0059| Learning rate: 0.0001\n",
            "Epoch: 004/020 | Batch 1800/2550 | Loss: 0.0021| Learning rate: 0.0001\n",
            "Epoch: 004/020 | Batch 1900/2550 | Loss: 0.0078| Learning rate: 0.0001\n",
            "Epoch: 004/020 | Batch 2000/2550 | Loss: 0.0059| Learning rate: 0.0001\n",
            "Epoch: 004/020 | Batch 2100/2550 | Loss: 0.0118| Learning rate: 0.0001\n",
            "Epoch: 004/020 | Batch 2200/2550 | Loss: 0.0086| Learning rate: 0.0001\n",
            "Epoch: 004/020 | Batch 2300/2550 | Loss: 0.0028| Learning rate: 0.0001\n",
            "Epoch: 004/020 | Batch 2400/2550 | Loss: 0.0083| Learning rate: 0.0001\n",
            "Epoch: 004/020 | Batch 2500/2550 | Loss: 0.0149| Learning rate: 0.0001\n",
            "Epoch: 004/020 | Train AVG LOSS:  0.0071 | Accuracy_train:  0.7892 \n",
            "| Validation AVG LOSS:  0.0726 | Accuracy_val:  0.3314 \n",
            "Time elapsed: 71.44 min\n",
            "Epoch: 005/020 | Batch 0000/2550 | Loss: 0.0084| Learning rate: 0.0001\n",
            "Epoch: 005/020 | Batch 0100/2550 | Loss: 0.0096| Learning rate: 0.0001\n",
            "Epoch: 005/020 | Batch 0200/2550 | Loss: 0.0057| Learning rate: 0.0001\n",
            "Epoch: 005/020 | Batch 0300/2550 | Loss: 0.0104| Learning rate: 0.0001\n",
            "Epoch: 005/020 | Batch 0400/2550 | Loss: 0.0061| Learning rate: 0.0001\n",
            "Epoch: 005/020 | Batch 0500/2550 | Loss: 0.0076| Learning rate: 0.0001\n",
            "Epoch: 005/020 | Batch 0600/2550 | Loss: 0.0034| Learning rate: 0.0001\n",
            "Epoch: 005/020 | Batch 0700/2550 | Loss: 0.0223| Learning rate: 0.0001\n",
            "Epoch: 005/020 | Batch 0800/2550 | Loss: 0.0091| Learning rate: 0.0001\n",
            "Epoch: 005/020 | Batch 0900/2550 | Loss: 0.0076| Learning rate: 0.0001\n",
            "Epoch: 005/020 | Batch 1000/2550 | Loss: 0.0071| Learning rate: 0.0001\n",
            "Epoch: 005/020 | Batch 1100/2550 | Loss: 0.0035| Learning rate: 0.0001\n",
            "Epoch: 005/020 | Batch 1200/2550 | Loss: 0.0043| Learning rate: 0.0001\n",
            "Epoch: 005/020 | Batch 1300/2550 | Loss: 0.0072| Learning rate: 0.0001\n",
            "Epoch: 005/020 | Batch 1400/2550 | Loss: 0.0019| Learning rate: 0.0001\n",
            "Epoch: 005/020 | Batch 1500/2550 | Loss: 0.0055| Learning rate: 0.0001\n",
            "Epoch: 005/020 | Batch 1600/2550 | Loss: 0.0116| Learning rate: 0.0001\n",
            "Epoch: 005/020 | Batch 1700/2550 | Loss: 0.0020| Learning rate: 0.0001\n",
            "Epoch: 005/020 | Batch 1800/2550 | Loss: 0.0025| Learning rate: 0.0001\n",
            "Epoch: 005/020 | Batch 1900/2550 | Loss: 0.0135| Learning rate: 0.0001\n",
            "Epoch: 005/020 | Batch 2000/2550 | Loss: 0.0040| Learning rate: 0.0001\n",
            "Epoch: 005/020 | Batch 2100/2550 | Loss: 0.0061| Learning rate: 0.0001\n",
            "Epoch: 005/020 | Batch 2200/2550 | Loss: 0.0064| Learning rate: 0.0001\n",
            "Epoch: 005/020 | Batch 2300/2550 | Loss: 0.0135| Learning rate: 0.0001\n",
            "Epoch: 005/020 | Batch 2400/2550 | Loss: 0.0101| Learning rate: 0.0001\n",
            "Epoch: 005/020 | Batch 2500/2550 | Loss: 0.0030| Learning rate: 0.0001\n",
            "Epoch: 005/020 | Train AVG LOSS:  0.0062 | Accuracy_train:  0.8094 \n",
            "| Validation AVG LOSS:  0.0724 | Accuracy_val:  0.3396 \n",
            "Time elapsed: 94.04 min\n",
            "Epoch: 006/020 | Batch 0000/2550 | Loss: 0.0157| Learning rate: 0.0001\n",
            "Epoch: 006/020 | Batch 0100/2550 | Loss: 0.0080| Learning rate: 0.0001\n",
            "Epoch: 006/020 | Batch 0200/2550 | Loss: 0.0115| Learning rate: 0.0001\n",
            "Epoch: 006/020 | Batch 0300/2550 | Loss: 0.0047| Learning rate: 0.0001\n",
            "Epoch: 006/020 | Batch 0400/2550 | Loss: 0.0039| Learning rate: 0.0001\n",
            "Epoch: 006/020 | Batch 0500/2550 | Loss: 0.0009| Learning rate: 0.0001\n",
            "Epoch: 006/020 | Batch 0600/2550 | Loss: 0.0057| Learning rate: 0.0001\n",
            "Epoch: 006/020 | Batch 0700/2550 | Loss: 0.0050| Learning rate: 0.0001\n",
            "Epoch: 006/020 | Batch 0800/2550 | Loss: 0.0061| Learning rate: 0.0001\n",
            "Epoch: 006/020 | Batch 0900/2550 | Loss: 0.0060| Learning rate: 0.0001\n",
            "Epoch: 006/020 | Batch 1000/2550 | Loss: 0.0075| Learning rate: 0.0001\n",
            "Epoch: 006/020 | Batch 1100/2550 | Loss: 0.0033| Learning rate: 0.0001\n",
            "Epoch: 006/020 | Batch 1200/2550 | Loss: 0.0022| Learning rate: 0.0001\n",
            "Epoch: 006/020 | Batch 1300/2550 | Loss: 0.0019| Learning rate: 0.0001\n",
            "Epoch: 006/020 | Batch 1400/2550 | Loss: 0.0029| Learning rate: 0.0001\n",
            "Epoch: 006/020 | Batch 1500/2550 | Loss: 0.0061| Learning rate: 0.0001\n",
            "Epoch: 006/020 | Batch 1600/2550 | Loss: 0.0126| Learning rate: 0.0001\n",
            "Epoch: 006/020 | Batch 1700/2550 | Loss: 0.0104| Learning rate: 0.0001\n",
            "Epoch: 006/020 | Batch 1800/2550 | Loss: 0.0079| Learning rate: 0.0001\n",
            "Epoch: 006/020 | Batch 1900/2550 | Loss: 0.0051| Learning rate: 0.0001\n",
            "Epoch: 006/020 | Batch 2000/2550 | Loss: 0.0039| Learning rate: 0.0001\n",
            "Epoch: 006/020 | Batch 2100/2550 | Loss: 0.0034| Learning rate: 0.0001\n",
            "Epoch: 006/020 | Batch 2200/2550 | Loss: 0.0034| Learning rate: 0.0001\n",
            "Epoch: 006/020 | Batch 2300/2550 | Loss: 0.0050| Learning rate: 0.0001\n",
            "Epoch: 006/020 | Batch 2400/2550 | Loss: 0.0045| Learning rate: 0.0001\n",
            "Epoch: 006/020 | Batch 2500/2550 | Loss: 0.0043| Learning rate: 0.0001\n",
            "Epoch: 006/020 | Train AVG LOSS:  0.0040 | Accuracy_train:  0.8909 \n",
            "| Validation AVG LOSS:  0.0667 | Accuracy_val:  0.3453 \n",
            "Time elapsed: 116.62 min\n",
            "Epoch: 007/020 | Batch 0000/2550 | Loss: 0.0018| Learning rate: 0.0001\n",
            "Epoch: 007/020 | Batch 0100/2550 | Loss: 0.0107| Learning rate: 0.0001\n",
            "Epoch: 007/020 | Batch 0200/2550 | Loss: 0.0022| Learning rate: 0.0001\n",
            "Epoch: 007/020 | Batch 0300/2550 | Loss: 0.0095| Learning rate: 0.0001\n",
            "Epoch: 007/020 | Batch 0400/2550 | Loss: 0.0018| Learning rate: 0.0001\n",
            "Epoch: 007/020 | Batch 0500/2550 | Loss: 0.0052| Learning rate: 0.0001\n",
            "Epoch: 007/020 | Batch 0600/2550 | Loss: 0.0054| Learning rate: 0.0001\n",
            "Epoch: 007/020 | Batch 0700/2550 | Loss: 0.0034| Learning rate: 0.0001\n",
            "Epoch: 007/020 | Batch 0800/2550 | Loss: 0.0029| Learning rate: 0.0001\n",
            "Epoch: 007/020 | Batch 0900/2550 | Loss: 0.0089| Learning rate: 0.0001\n",
            "Epoch: 007/020 | Batch 1000/2550 | Loss: 0.0041| Learning rate: 0.0001\n",
            "Epoch: 007/020 | Batch 1100/2550 | Loss: 0.0004| Learning rate: 0.0001\n",
            "Epoch: 007/020 | Batch 1200/2550 | Loss: 0.0024| Learning rate: 0.0001\n",
            "Epoch: 007/020 | Batch 1300/2550 | Loss: 0.0022| Learning rate: 0.0001\n",
            "Epoch: 007/020 | Batch 1400/2550 | Loss: 0.0021| Learning rate: 0.0001\n",
            "Epoch: 007/020 | Batch 1500/2550 | Loss: 0.0006| Learning rate: 0.0001\n",
            "Epoch: 007/020 | Batch 1600/2550 | Loss: 0.0092| Learning rate: 0.0001\n",
            "Epoch: 007/020 | Batch 1700/2550 | Loss: 0.0100| Learning rate: 0.0001\n",
            "Epoch: 007/020 | Batch 1800/2550 | Loss: 0.0019| Learning rate: 0.0001\n",
            "Epoch: 007/020 | Batch 1900/2550 | Loss: 0.0035| Learning rate: 0.0001\n",
            "Epoch: 007/020 | Batch 2000/2550 | Loss: 0.0043| Learning rate: 0.0001\n",
            "Epoch: 007/020 | Batch 2100/2550 | Loss: 0.0025| Learning rate: 0.0001\n",
            "Epoch: 007/020 | Batch 2200/2550 | Loss: 0.0073| Learning rate: 0.0001\n",
            "Epoch: 007/020 | Batch 2300/2550 | Loss: 0.0041| Learning rate: 0.0001\n",
            "Epoch: 007/020 | Batch 2400/2550 | Loss: 0.0038| Learning rate: 0.0001\n",
            "Epoch: 007/020 | Batch 2500/2550 | Loss: 0.0036| Learning rate: 0.0001\n",
            "Epoch: 007/020 | Train AVG LOSS:  0.0037 | Accuracy_train:  0.9023 \n",
            "| Validation AVG LOSS:  0.0650 | Accuracy_val:  0.3635 \n",
            "Time elapsed: 139.21 min\n",
            "Epoch: 008/020 | Batch 0000/2550 | Loss: 0.0039| Learning rate: 0.0001\n",
            "Epoch: 008/020 | Batch 0100/2550 | Loss: 0.0061| Learning rate: 0.0001\n",
            "Epoch: 008/020 | Batch 0200/2550 | Loss: 0.0058| Learning rate: 0.0001\n",
            "Epoch: 008/020 | Batch 0300/2550 | Loss: 0.0092| Learning rate: 0.0001\n",
            "Epoch: 008/020 | Batch 0400/2550 | Loss: 0.0070| Learning rate: 0.0001\n",
            "Epoch: 008/020 | Batch 0500/2550 | Loss: 0.0007| Learning rate: 0.0001\n",
            "Epoch: 008/020 | Batch 0600/2550 | Loss: 0.0060| Learning rate: 0.0001\n",
            "Epoch: 008/020 | Batch 0700/2550 | Loss: 0.0055| Learning rate: 0.0001\n",
            "Epoch: 008/020 | Batch 0800/2550 | Loss: 0.0042| Learning rate: 0.0001\n",
            "Epoch: 008/020 | Batch 0900/2550 | Loss: 0.0016| Learning rate: 0.0001\n",
            "Epoch: 008/020 | Batch 1000/2550 | Loss: 0.0034| Learning rate: 0.0001\n",
            "Epoch: 008/020 | Batch 1100/2550 | Loss: 0.0017| Learning rate: 0.0001\n",
            "Epoch: 008/020 | Batch 1200/2550 | Loss: 0.0049| Learning rate: 0.0001\n",
            "Epoch: 008/020 | Batch 1300/2550 | Loss: 0.0014| Learning rate: 0.0001\n",
            "Epoch: 008/020 | Batch 1400/2550 | Loss: 0.0326| Learning rate: 0.0001\n",
            "Epoch: 008/020 | Batch 1500/2550 | Loss: 0.0024| Learning rate: 0.0001\n",
            "Epoch: 008/020 | Batch 1600/2550 | Loss: 0.0085| Learning rate: 0.0001\n",
            "Epoch: 008/020 | Batch 1700/2550 | Loss: 0.0032| Learning rate: 0.0001\n",
            "Epoch: 008/020 | Batch 1800/2550 | Loss: 0.0040| Learning rate: 0.0001\n",
            "Epoch: 008/020 | Batch 1900/2550 | Loss: 0.0024| Learning rate: 0.0001\n",
            "Epoch: 008/020 | Batch 2000/2550 | Loss: 0.0033| Learning rate: 0.0001\n",
            "Epoch: 008/020 | Batch 2100/2550 | Loss: 0.0016| Learning rate: 0.0001\n",
            "Epoch: 008/020 | Batch 2200/2550 | Loss: 0.0066| Learning rate: 0.0001\n",
            "Epoch: 008/020 | Batch 2300/2550 | Loss: 0.0031| Learning rate: 0.0001\n",
            "Epoch: 008/020 | Batch 2400/2550 | Loss: 0.0048| Learning rate: 0.0001\n",
            "Epoch: 008/020 | Batch 2500/2550 | Loss: 0.0015| Learning rate: 0.0001\n",
            "Epoch: 008/020 | Train AVG LOSS:  0.0031 | Accuracy_train:  0.9300 \n",
            "| Validation AVG LOSS:  0.0715 | Accuracy_val:  0.3585 \n",
            "Time elapsed: 161.71 min\n",
            "Epoch: 009/020 | Batch 0000/2550 | Loss: 0.0011| Learning rate: 0.0001\n",
            "Epoch: 009/020 | Batch 0100/2550 | Loss: 0.0036| Learning rate: 0.0001\n",
            "Epoch: 009/020 | Batch 0200/2550 | Loss: 0.0027| Learning rate: 0.0001\n",
            "Epoch: 009/020 | Batch 0300/2550 | Loss: 0.0010| Learning rate: 0.0001\n",
            "Epoch: 009/020 | Batch 0400/2550 | Loss: 0.0033| Learning rate: 0.0001\n",
            "Epoch: 009/020 | Batch 0500/2550 | Loss: 0.0033| Learning rate: 0.0001\n",
            "Epoch: 009/020 | Batch 0600/2550 | Loss: 0.0062| Learning rate: 0.0001\n",
            "Epoch: 009/020 | Batch 0700/2550 | Loss: 0.0038| Learning rate: 0.0001\n",
            "Epoch: 009/020 | Batch 0800/2550 | Loss: 0.0050| Learning rate: 0.0001\n",
            "Epoch: 009/020 | Batch 0900/2550 | Loss: 0.0020| Learning rate: 0.0001\n",
            "Epoch: 009/020 | Batch 1000/2550 | Loss: 0.0081| Learning rate: 0.0001\n",
            "Epoch: 009/020 | Batch 1100/2550 | Loss: 0.0010| Learning rate: 0.0001\n",
            "Epoch: 009/020 | Batch 1200/2550 | Loss: 0.0022| Learning rate: 0.0001\n",
            "Epoch: 009/020 | Batch 1300/2550 | Loss: 0.0017| Learning rate: 0.0001\n",
            "Epoch: 009/020 | Batch 1400/2550 | Loss: 0.0011| Learning rate: 0.0001\n",
            "Epoch: 009/020 | Batch 1500/2550 | Loss: 0.0032| Learning rate: 0.0001\n",
            "Epoch: 009/020 | Batch 1600/2550 | Loss: 0.0018| Learning rate: 0.0001\n",
            "Epoch: 009/020 | Batch 1700/2550 | Loss: 0.0020| Learning rate: 0.0001\n",
            "Epoch: 009/020 | Batch 1800/2550 | Loss: 0.0045| Learning rate: 0.0001\n",
            "Epoch: 009/020 | Batch 1900/2550 | Loss: 0.0036| Learning rate: 0.0001\n",
            "Epoch: 009/020 | Batch 2000/2550 | Loss: 0.0035| Learning rate: 0.0001\n",
            "Epoch: 009/020 | Batch 2100/2550 | Loss: 0.0021| Learning rate: 0.0001\n",
            "Epoch: 009/020 | Batch 2200/2550 | Loss: 0.0024| Learning rate: 0.0001\n",
            "Epoch: 009/020 | Batch 2300/2550 | Loss: 0.0023| Learning rate: 0.0001\n",
            "Epoch: 009/020 | Batch 2400/2550 | Loss: 0.0061| Learning rate: 0.0001\n",
            "Epoch: 009/020 | Batch 2500/2550 | Loss: 0.0020| Learning rate: 0.0001\n",
            "Epoch: 009/020 | Train AVG LOSS:  0.0030 | Accuracy_train:  0.9299 \n",
            "| Validation AVG LOSS:  0.0759 | Accuracy_val:  0.3327 \n",
            "Time elapsed: 184.20 min\n",
            "Epoch: 010/020 | Batch 0000/2550 | Loss: 0.0011| Learning rate: 0.0001\n",
            "Epoch: 010/020 | Batch 0100/2550 | Loss: 0.0040| Learning rate: 0.0001\n",
            "Epoch: 010/020 | Batch 0200/2550 | Loss: 0.0022| Learning rate: 0.0001\n",
            "Epoch: 010/020 | Batch 0300/2550 | Loss: 0.0014| Learning rate: 0.0001\n",
            "Epoch: 010/020 | Batch 0400/2550 | Loss: 0.0073| Learning rate: 0.0001\n",
            "Epoch: 010/020 | Batch 0500/2550 | Loss: 0.0045| Learning rate: 0.0001\n",
            "Epoch: 010/020 | Batch 0600/2550 | Loss: 0.0036| Learning rate: 0.0001\n",
            "Epoch: 010/020 | Batch 0700/2550 | Loss: 0.0009| Learning rate: 0.0001\n",
            "Epoch: 010/020 | Batch 0800/2550 | Loss: 0.0024| Learning rate: 0.0001\n",
            "Epoch: 010/020 | Batch 0900/2550 | Loss: 0.0014| Learning rate: 0.0001\n",
            "Epoch: 010/020 | Batch 1000/2550 | Loss: 0.0042| Learning rate: 0.0001\n",
            "Epoch: 010/020 | Batch 1100/2550 | Loss: 0.0018| Learning rate: 0.0001\n",
            "Epoch: 010/020 | Batch 1200/2550 | Loss: 0.0023| Learning rate: 0.0001\n",
            "Epoch: 010/020 | Batch 1300/2550 | Loss: 0.0016| Learning rate: 0.0001\n",
            "Epoch: 010/020 | Batch 1400/2550 | Loss: 0.0034| Learning rate: 0.0001\n",
            "Epoch: 010/020 | Batch 1500/2550 | Loss: 0.0110| Learning rate: 0.0001\n",
            "Epoch: 010/020 | Batch 1600/2550 | Loss: 0.0027| Learning rate: 0.0001\n",
            "Epoch: 010/020 | Batch 1700/2550 | Loss: 0.0044| Learning rate: 0.0001\n",
            "Epoch: 010/020 | Batch 1800/2550 | Loss: 0.0061| Learning rate: 0.0001\n",
            "Epoch: 010/020 | Batch 1900/2550 | Loss: 0.0012| Learning rate: 0.0001\n",
            "Epoch: 010/020 | Batch 2000/2550 | Loss: 0.0044| Learning rate: 0.0001\n",
            "Epoch: 010/020 | Batch 2100/2550 | Loss: 0.0030| Learning rate: 0.0001\n",
            "Epoch: 010/020 | Batch 2200/2550 | Loss: 0.0033| Learning rate: 0.0001\n",
            "Epoch: 010/020 | Batch 2300/2550 | Loss: 0.0045| Learning rate: 0.0001\n",
            "Epoch: 010/020 | Batch 2400/2550 | Loss: 0.0009| Learning rate: 0.0001\n",
            "Epoch: 010/020 | Batch 2500/2550 | Loss: 0.0021| Learning rate: 0.0001\n",
            "Epoch: 010/020 | Train AVG LOSS:  0.0028 | Accuracy_train:  0.9386 \n",
            "| Validation AVG LOSS:  0.0672 | Accuracy_val:  0.3113 \n",
            "Time elapsed: 206.82 min\n",
            "Epoch: 011/020 | Batch 0000/2550 | Loss: 0.0038| Learning rate: 0.0001\n",
            "Epoch: 011/020 | Batch 0100/2550 | Loss: 0.0010| Learning rate: 0.0001\n",
            "Epoch: 011/020 | Batch 0200/2550 | Loss: 0.0010| Learning rate: 0.0001\n",
            "Epoch: 011/020 | Batch 0300/2550 | Loss: 0.0017| Learning rate: 0.0001\n",
            "Epoch: 011/020 | Batch 0400/2550 | Loss: 0.0024| Learning rate: 0.0001\n",
            "Epoch: 011/020 | Batch 0500/2550 | Loss: 0.0014| Learning rate: 0.0001\n",
            "Epoch: 011/020 | Batch 0600/2550 | Loss: 0.0019| Learning rate: 0.0001\n",
            "Epoch: 011/020 | Batch 0700/2550 | Loss: 0.0016| Learning rate: 0.0001\n",
            "Epoch: 011/020 | Batch 0800/2550 | Loss: 0.0040| Learning rate: 0.0001\n",
            "Epoch: 011/020 | Batch 0900/2550 | Loss: 0.0036| Learning rate: 0.0001\n",
            "Epoch: 011/020 | Batch 1000/2550 | Loss: 0.0016| Learning rate: 0.0001\n",
            "Epoch: 011/020 | Batch 1100/2550 | Loss: 0.0008| Learning rate: 0.0001\n",
            "Epoch: 011/020 | Batch 1200/2550 | Loss: 0.0025| Learning rate: 0.0001\n",
            "Epoch: 011/020 | Batch 1300/2550 | Loss: 0.0030| Learning rate: 0.0001\n",
            "Epoch: 011/020 | Batch 1400/2550 | Loss: 0.0016| Learning rate: 0.0001\n",
            "Epoch: 011/020 | Batch 1500/2550 | Loss: 0.0019| Learning rate: 0.0001\n",
            "Epoch: 011/020 | Batch 1600/2550 | Loss: 0.0031| Learning rate: 0.0001\n",
            "Epoch: 011/020 | Batch 1700/2550 | Loss: 0.0020| Learning rate: 0.0001\n",
            "Epoch: 011/020 | Batch 1800/2550 | Loss: 0.0009| Learning rate: 0.0001\n",
            "Epoch: 011/020 | Batch 1900/2550 | Loss: 0.0052| Learning rate: 0.0001\n",
            "Epoch: 011/020 | Batch 2000/2550 | Loss: 0.0009| Learning rate: 0.0001\n",
            "Epoch: 011/020 | Batch 2100/2550 | Loss: 0.0012| Learning rate: 0.0001\n",
            "Epoch: 011/020 | Batch 2200/2550 | Loss: 0.0015| Learning rate: 0.0001\n",
            "Epoch: 011/020 | Batch 2300/2550 | Loss: 0.0009| Learning rate: 0.0001\n",
            "Epoch: 011/020 | Batch 2400/2550 | Loss: 0.0015| Learning rate: 0.0001\n",
            "Epoch: 011/020 | Batch 2500/2550 | Loss: 0.0033| Learning rate: 0.0001\n",
            "Epoch: 011/020 | Train AVG LOSS:  0.0022 | Accuracy_train:  0.9590 \n",
            "| Validation AVG LOSS:  0.0725 | Accuracy_val:  0.3094 \n",
            "Time elapsed: 229.46 min\n",
            "Epoch: 012/020 | Batch 0000/2550 | Loss: 0.0014| Learning rate: 0.0001\n",
            "Epoch: 012/020 | Batch 0100/2550 | Loss: 0.0012| Learning rate: 0.0001\n",
            "Epoch: 012/020 | Batch 0200/2550 | Loss: 0.0009| Learning rate: 0.0001\n",
            "Epoch: 012/020 | Batch 0300/2550 | Loss: 0.0008| Learning rate: 0.0001\n",
            "Epoch: 012/020 | Batch 0400/2550 | Loss: 0.0023| Learning rate: 0.0001\n",
            "Epoch: 012/020 | Batch 0500/2550 | Loss: 0.0020| Learning rate: 0.0001\n",
            "Epoch: 012/020 | Batch 0600/2550 | Loss: 0.0026| Learning rate: 0.0001\n",
            "Epoch: 012/020 | Batch 0700/2550 | Loss: 0.0021| Learning rate: 0.0001\n",
            "Epoch: 012/020 | Batch 0800/2550 | Loss: 0.0014| Learning rate: 0.0001\n",
            "Epoch: 012/020 | Batch 0900/2550 | Loss: 0.0027| Learning rate: 0.0001\n",
            "Epoch: 012/020 | Batch 1000/2550 | Loss: 0.0033| Learning rate: 0.0001\n",
            "Epoch: 012/020 | Batch 1100/2550 | Loss: 0.0051| Learning rate: 0.0001\n",
            "Epoch: 012/020 | Batch 1200/2550 | Loss: 0.0028| Learning rate: 0.0001\n",
            "Epoch: 012/020 | Batch 1300/2550 | Loss: 0.0020| Learning rate: 0.0001\n",
            "Epoch: 012/020 | Batch 1400/2550 | Loss: 0.0017| Learning rate: 0.0001\n",
            "Epoch: 012/020 | Batch 1500/2550 | Loss: 0.0006| Learning rate: 0.0001\n",
            "Epoch: 012/020 | Batch 1600/2550 | Loss: 0.0015| Learning rate: 0.0001\n",
            "Epoch: 012/020 | Batch 1700/2550 | Loss: 0.0020| Learning rate: 0.0001\n",
            "Epoch: 012/020 | Batch 1800/2550 | Loss: 0.0044| Learning rate: 0.0001\n",
            "Epoch: 012/020 | Batch 1900/2550 | Loss: 0.0023| Learning rate: 0.0001\n",
            "Epoch: 012/020 | Batch 2000/2550 | Loss: 0.0017| Learning rate: 0.0001\n",
            "Epoch: 012/020 | Batch 2100/2550 | Loss: 0.0021| Learning rate: 0.0001\n",
            "Epoch: 012/020 | Batch 2200/2550 | Loss: 0.0037| Learning rate: 0.0001\n",
            "Epoch: 012/020 | Batch 2300/2550 | Loss: 0.0015| Learning rate: 0.0001\n",
            "Epoch: 012/020 | Batch 2400/2550 | Loss: 0.0010| Learning rate: 0.0001\n",
            "Epoch: 012/020 | Batch 2500/2550 | Loss: 0.0009| Learning rate: 0.0001\n"
          ]
        }
      ]
    }
  ]
}
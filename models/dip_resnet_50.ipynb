{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Открытие гугл диска"
      ],
      "metadata": {
        "id": "dyPHVOPOJF14"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x0d_GduAFRHf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f08719ab-c296-4141-e3df-6abc610c8763"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import os.path\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import random\n",
        "from torchvision import transforms\n",
        "import math\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import time\n",
        "from torch.amp import autocast, GradScaler"
      ],
      "metadata": {
        "id": "8SxdtPCEJK6q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Дата лоадер"
      ],
      "metadata": {
        "id": "XMGTzoGQJMfo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_pathes(base_folder ):\n",
        "    paths = []\n",
        "    for i in range(11):\n",
        "        folder_name = f'{i:02d}'\n",
        "        image_folder = os.path.join(base_folder, folder_name, 'image_0')\n",
        "        movements_file = os.path.join(base_folder, folder_name, f'{folder_name}.txt')\n",
        "        paths.append((image_folder,movements_file))\n",
        "    return paths\n",
        "\n",
        "def get_data(batch_size, base_folder = '/content/drive/MyDrive'):\n",
        "    paths = get_pathes(base_folder)\n",
        "\n",
        "    # Выбор папок для тренировочных, валидационных и тестовых данных\n",
        "    validation_folder = paths[-2]  # Предпоследняя папка\n",
        "    test_folder = paths[-1]        # Последняя папка\n",
        "    train_folders = paths[:-2]     # Все остальные папки\n",
        "\n",
        "    # Чтение движений из файла\n",
        "    def create_movements(movements_file):\n",
        "        movements = []\n",
        "        with open(movements_file, 'r') as f:\n",
        "            for line in f:\n",
        "                data = list(map(float, line.strip().split()))\n",
        "                movements.append(data)\n",
        "        return movements\n",
        "\n",
        "    # movements = create_movements(movements_file)\n",
        "\n",
        "    # Преобразование движений\n",
        "    # def transform_mov(movements):\n",
        "    #     new_movements = []\n",
        "    #     for matrix in movements:\n",
        "    #         new_matrix = [\n",
        "    #             matrix[0:4],\n",
        "    #             matrix[4:8],\n",
        "    #             matrix[8:12],\n",
        "    #             [0, 0, 0, 1]\n",
        "    #         ]\n",
        "    #         new_matrix = torch.tensor(new_matrix, dtype=torch.float32)\n",
        "    #         new_movements.append(new_matrix)\n",
        "\n",
        "    #     res_movements = []\n",
        "    #     for i in range(len(new_movements) - 1):\n",
        "    #         T1_inv = torch.linalg.inv(new_movements[i])\n",
        "    #         T_rel = torch.matmul(new_movements[i + 1], T1_inv)\n",
        "    #         res_matrix = T_rel[:3]\n",
        "    #         res_matrix = torch.cat([res_matrix[0], res_matrix[1], res_matrix[2]])\n",
        "    #         res_movements.append(res_matrix)\n",
        "\n",
        "\n",
        "    #     return res_movements\n",
        "\n",
        "    # def transform_mov(movements):\n",
        "    #     res_movements = []\n",
        "    #     for i in range(len(movements)-1):\n",
        "    #         m1 = torch.tensor(movements[i])\n",
        "    #         m2 = torch.tensor(movements[i+1])\n",
        "    #         res = m2-m1\n",
        "    #         res_movements.append(res)\n",
        "    #     return res_movements\n",
        "    step = 1\n",
        "\n",
        "    def transform_mov(movements):\n",
        "        res_movements = []\n",
        "        # for i in range(len(movements)-1):\n",
        "        for i in range(len(movements)-step):\n",
        "            m1 = torch.tensor(movements[i])\n",
        "            m2 = torch.tensor(movements[i+step])\n",
        "            res = m2-m1\n",
        "            distance = math.sqrt(pow(res[3],2) + pow(res[7],2) + pow(res[11],2))\n",
        "            res_movements.append([distance ])\n",
        "        return res_movements\n",
        "\n",
        "\n",
        "\n",
        "    # movements = transform_mov(movements)\n",
        "\n",
        "    # Оптимизированный класс Dataset\n",
        "    class ImageDataset(Dataset):\n",
        "        def __init__(self, image_folder, movements, transform=None ):\n",
        "            # Получаем список всех файлов .png и сортируем их\n",
        "            all_image_files = sorted([os.path.join(image_folder, file) for file in os.listdir(image_folder) if file.endswith('.png')])\n",
        "            self.image_files = all_image_files\n",
        "            self.movements = movements\n",
        "            self.transform = transform\n",
        "\n",
        "        def __len__(self):\n",
        "            # Возвращаем минимальную длину между движениями и изображениями - 1\n",
        "            return min(len(self.movements), len(self.image_files) - 1)\n",
        "\n",
        "\n",
        "        def __getitem__(self, idx):\n",
        "            image1_path = self.image_files[idx]\n",
        "            # image2_path = self.image_files[idx + 1]\n",
        "            image2_path = self.image_files[idx + step]\n",
        "\n",
        "            image1 = Image.open(image1_path).convert('L')\n",
        "            image2 = Image.open(image2_path).convert('L')\n",
        "\n",
        "            if self.transform:\n",
        "                image1 = self.transform(image1)\n",
        "                image2 = self.transform(image2)\n",
        "\n",
        "            movement = self.movements[idx].clone().detach().float()if isinstance(self.movements[idx], torch.Tensor) else torch.tensor(self.movements[idx], dtype=torch.float32)\n",
        "            return (image1, image2), movement\n",
        "\n",
        "\n",
        "    # Трансформации для уменьшения использования памяти\n",
        "    transform = transforms.Compose([\n",
        "        transforms.Grayscale(),\n",
        "        # transforms.Resize((376,1240)),\n",
        "        transforms.CenterCrop((370,1226)),\n",
        "        # transforms.Resize((180,320)),\n",
        "        transforms.ToTensor()\n",
        "    ])\n",
        "\n",
        "    def load_dataset(folders):\n",
        "        images, movements = [], []\n",
        "        for image_folder, movements_file in folders:\n",
        "            mov = create_movements(movements_file)\n",
        "            mov = transform_mov(mov)\n",
        "            movements.append(mov)\n",
        "            images.append(image_folder)\n",
        "        return images, movements\n",
        "\n",
        "    train_images, train_movements = load_dataset(train_folders)\n",
        "    train_dataset = []\n",
        "    for index,image_folder in enumerate(train_images):\n",
        "        dataset = ImageDataset(image_folder, train_movements[index], transform=transform)\n",
        "        train_dataset.append(dataset)\n",
        "    train_dataset = torch.utils.data.ConcatDataset(train_dataset)\n",
        "\n",
        "    # Загрузка валидационных данных\n",
        "    val_images, val_movements = load_dataset([validation_folder])\n",
        "    val_dataset = ImageDataset(val_images[0], val_movements[0], transform=transform)\n",
        "\n",
        "\n",
        "   # Загрузка тестовых данных\n",
        "    test_images, test_movements = load_dataset([test_folder])\n",
        "    test_dataset = ImageDataset(test_images[0], test_movements[0], transform=transform)\n",
        "\n",
        "    # DataLoaders\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle = True, num_workers=4, pin_memory=True)\n",
        "    validation_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle = False, num_workers=4, pin_memory=True)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle = False, num_workers=4, pin_memory=True)\n",
        "\n",
        "    return train_loader, validation_loader, test_loader\n"
      ],
      "metadata": {
        "id": "y2e9tPaxJMBu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "EVALUATION\n"
      ],
      "metadata": {
        "id": "9b-omipcJO-t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def set_all_seeds(seed):\n",
        "    os.environ[\"PL_GLOBAL_SEED\"] = str(seed)\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "def compute_accuracy(model, data_loader, device):\n",
        "    model.eval()\n",
        "    total_loss = 0.0\n",
        "    num_batches = 0\n",
        "    correct_predictions = 0   # Счетчик корректных предсказаний\n",
        "    total_samples = 0         # Общее количество элементов\n",
        "\n",
        "    criterion = torch.nn.MSELoss()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, ((feature1, feature2),targets) in enumerate(data_loader):\n",
        "            feature1 = feature1.to(device)\n",
        "            feature2 = feature2.to(device)\n",
        "            targets = targets.to(device)\n",
        "            with autocast():\n",
        "                predicted_y = model(feature1, feature2)\n",
        "\n",
        "            # Вычисление ошибки\n",
        "            loss = criterion(predicted_y, targets)\n",
        "            total_loss += loss.item()\n",
        "            num_batches += 1\n",
        "\n",
        "            # Проверка критерия качества\n",
        "            relative_difference = torch.abs((predicted_y - targets))  # Расчет абсолютного отклонения\n",
        "            valid_predictions = torch.all(relative_difference <= 0.1, dim=1)    # Условие: отклонение <= 10 см\n",
        "            correct_predictions += torch.sum(valid_predictions).item()\n",
        "            total_samples = total_samples + targets.size(0)\n",
        "\n",
        "\n",
        "    avg_loss = total_loss / num_batches             # Расчет средней ошибки\n",
        "    accuracy = correct_predictions / total_samples  # Расчет доли верных предсказаний\n",
        "    return avg_loss, accuracy"
      ],
      "metadata": {
        "id": "lVj3ME_4JQdS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model"
      ],
      "metadata": {
        "id": "KSs02KT2JR90"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchvision.models import resnet50, ResNet50_Weights"
      ],
      "metadata": {
        "id": "Wq9jFuiiJp-a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Network(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        # Загружаем предобученный ResNet101\n",
        "        weights = ResNet50_Weights.DEFAULT\n",
        "        base_model = resnet50(weights=weights)\n",
        "\n",
        "        # Заменяем первый conv слой для 1-канального изображения\n",
        "        self.backbone = nn.Sequential()\n",
        "        conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
        "        # Копируем и усредняем веса по каналам\n",
        "        with torch.no_grad():\n",
        "            conv1.weight[:] = base_model.conv1.weight.mean(dim=1, keepdim=True)\n",
        "        self.backbone.add_module(\"conv1\", conv1)\n",
        "\n",
        "        # Добавляем остальные блоки ResNet до avgpool (исключая fc)\n",
        "        for name, module in list(base_model.named_children())[1:-2]:\n",
        "            self.backbone.add_module(name, module)\n",
        "\n",
        "        # Размерность последовательности: [B, 2048, 12, 39] → [B, 468, 2048]\n",
        "        # self.lstm = nn.LSTM(\n",
        "        #     input_size=2048,\n",
        "        #     hidden_size=hidden_size,\n",
        "        #     num_layers=num_layers,\n",
        "        #     batch_first=True,\n",
        "        #     bidirectional=bidirectional\n",
        "        # )\n",
        "\n",
        "        self.lstm1 = nn.LSTMCell(input_size = 2048*3*1, hidden_size=128)\n",
        "        self.lstm2 = nn.LSTMCell(input_size = 2048*3*1, hidden_size=128)\n",
        "\n",
        "        self.classifier = nn.Linear(128, 1)\n",
        "\n",
        "    # def forward(self, x):\n",
        "    #     # x: [B, 1, 370, 1226]\n",
        "    #     x = self.backbone(x)              # [B, 2048, 12, 39]\n",
        "    #     x = x.permute(0, 2, 3, 1)         # [B, 12, 39, 2048]\n",
        "    #     x = x.reshape(x.size(0), -1, 2048)  # [B, 468, 2048]\n",
        "    #     lstm_out, _ = self.lstm(x)        # [B, 468, hidden]\n",
        "    #     out = self.classifier(lstm_out[:, -1, :])  # финальный временной шаг\n",
        "    #     return out\n",
        "\n",
        "    def forward(self,image1,image2):\n",
        "        out_features1 = self.backbone(image1)\n",
        "        out_features2 = self.backbone(image2)\n",
        "\n",
        "        out_features1 = torch.nn.functional.adaptive_avg_pool2d(out_features1, (1, 3))  # [B, C, 3, 10]\n",
        "        out_features1 = out_features1.view(out_features1.size(0), -1)  # [B, C * 3 * 10]\n",
        "\n",
        "        out_features2 = torch.nn.functional.adaptive_avg_pool2d(out_features2, (1, 3))\n",
        "        out_features2 = out_features2.view(out_features2.size(0), -1)\n",
        "        batch_size = out_features1.size(0)\n",
        "\n",
        "        # Инициализируем скрытые состояния и состояния ячеек для lstm1\n",
        "        h_t1 = torch.zeros(batch_size, self.lstm1.hidden_size, device=out_features1.device)\n",
        "        c_t1 = torch.zeros(batch_size, self.lstm1.hidden_size, device=out_features1.device)\n",
        "\n",
        "        # Пропускаем признаки первой картинки через lstm1 (один временной шаг)\n",
        "        h_t1_next, c_t1_next = self.lstm1(out_features1, (h_t1, c_t1))\n",
        "        h_t2_next, c_t2_next = self.lstm2(out_features2, (h_t1_next, c_t1_next))\n",
        "        out = self.classifier(h_t2_next) # [batch_size, 1]\n",
        "        return out\n"
      ],
      "metadata": {
        "id": "CcYFIAWsJTHI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train\n"
      ],
      "metadata": {
        "id": "ahE9c2jNJU0m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, num_epochs, last_epoch, train_loader,\n",
        "                validation_loader, test_loader,optimizer, device,scheduler,\n",
        "          checkpoint_dict,scaler):\n",
        "    start_time = time.time()\n",
        "    minibatch_loss_list, train_loss_list, valid_loss_list = [] ,[], []\n",
        "    criterion = torch.nn.MSELoss()\n",
        "\n",
        "    for epoch in range(last_epoch, num_epochs):\n",
        "        model.train()\n",
        "        for batch_idx,((feature1,feature2),targets) in enumerate(train_loader):\n",
        "            feature1 = feature1.to(device)\n",
        "            feature2 = feature2.to(device)\n",
        "            targets = targets.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            # <<< Mixed precision begins\n",
        "            with autocast():\n",
        "                predicted_y = model(feature1,feature2)\n",
        "                loss = criterion(predicted_y, targets)\n",
        "\n",
        "            #  Зворотній прохід\n",
        "            scaler.scale(loss).backward()\n",
        "            scaler.step(optimizer)\n",
        "            scaler.update()\n",
        "\n",
        "            minibatch_loss_list.append(loss.item())\n",
        "            if batch_idx %100 == 0:\n",
        "                print(f'Epoch: {epoch+1:03d}/{num_epochs:03d} '\n",
        "                      f'| Batch {batch_idx:04d}/{len(train_loader):04d} '\n",
        "                      f'| Loss: {loss:.4f}'\n",
        "                      f'| Learning rate: {optimizer.param_groups[0][\"lr\"]}')\n",
        "        # scheduler.step()\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            avg_loss_train, accuracy_train = compute_accuracy(model, train_loader, device)\n",
        "            avg_loss_val, accuracy_val = compute_accuracy(model, validation_loader, device)\n",
        "            # scheduler.step( avg_loss_val)\n",
        "            print(f'Epoch: {epoch+1:03d}/{num_epochs:03d} '\n",
        "                  f'| Train AVG LOSS: {avg_loss_train: .4f} | Accuracy_train: {accuracy_train: .4f} \\n'\n",
        "                  f'| Validation AVG LOSS: {avg_loss_val: .4f} | Accuracy_val: {accuracy_val: .4f} ')\n",
        "            train_loss_list.append(avg_loss_train)\n",
        "            valid_loss_list.append(avg_loss_val)\n",
        "\n",
        "        scheduler.step()\n",
        "\n",
        "        # checkpoint = {\n",
        "        #     'model_state_dict': model.state_dict(),\n",
        "        #     'optimizer_state_dict': optimizer.state_dict()\n",
        "        # }\n",
        "\n",
        "        checkpoint_dict['state_model'] = model.state_dict()\n",
        "        checkpoint_dict['state_opt'] =  optimizer.state_dict()\n",
        "        checkpoint_dict[ 'state_scheduler'] = scheduler.state_dict()\n",
        "        checkpoint_dict['train_loss'] = avg_loss_train\n",
        "        checkpoint_dict['val_loss'] = avg_loss_val\n",
        "        checkpoint_dict['train_acc'] = accuracy_train\n",
        "        checkpoint_dict['val_acc'] =  accuracy_val\n",
        "        checkpoint_dict['EPOCHS'] = num_epochs\n",
        "        checkpoint_dict['current_epoch'] = epoch + 1\n",
        "        checkpoint_dict[ 'learning'] = optimizer.param_groups[0][\"lr\"]\n",
        "\n",
        "        path = directory_path + str(epoch + 1) + '_epoch.pth'\n",
        "        torch.save(checkpoint_dict, path)\n",
        "        # path = '/content/drive/My Drive/checkpoint_model_3.pth'\n",
        "        # torch.save(checkpoint, path)\n",
        "\n",
        "        elapsed = (time.time() - start_time)/60\n",
        "        print(f'Time elapsed: {elapsed:.2f} min')\n",
        "\n",
        "    elapsed = (time.time() - start_time)/60\n",
        "    print(f'Total Training Time: {elapsed:.2f} min')\n",
        "\n",
        "    avg_loss_test, accuracy_test = compute_accuracy(model, test_loader,device)\n",
        "    print(f'Test AVG LOSS: {avg_loss_test: .4f} | Accuracy_test: {accuracy_test: .4f}')\n",
        "\n",
        "    return minibatch_loss_list, train_loss_list, valid_loss_list\n"
      ],
      "metadata": {
        "id": "WpkTwGZXJWQb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Описание модели, оптимизатора, шедулера"
      ],
      "metadata": {
        "id": "raMr_qAwJYVf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "txt_model_opt_shed = '''\n",
        "class Network(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        # Загружаем предобученный ResNet101\n",
        "        weights = ResNet50_Weights.DEFAULT\n",
        "        base_model = resnet50(weights=weights)\n",
        "\n",
        "        # Заменяем первый conv слой для 1-канального изображения\n",
        "        self.backbone = nn.Sequential()\n",
        "        conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
        "        # Копируем и усредняем веса по каналам\n",
        "        with torch.no_grad():\n",
        "            conv1.weight[:] = base_model.conv1.weight.mean(dim=1, keepdim=True)\n",
        "        self.backbone.add_module(\"conv1\", conv1)\n",
        "\n",
        "        # Добавляем остальные блоки ResNet до avgpool (исключая fc)\n",
        "        for name, module in list(base_model.named_children())[1:-2]:\n",
        "            self.backbone.add_module(name, module)\n",
        "\n",
        "        # Размерность последовательности: [B, 2048, 12, 39] → [B, 468, 2048]\n",
        "        # self.lstm = nn.LSTM(\n",
        "        #     input_size=2048,\n",
        "        #     hidden_size=hidden_size,\n",
        "        #     num_layers=num_layers,\n",
        "        #     batch_first=True,\n",
        "        #     bidirectional=bidirectional\n",
        "        # )\n",
        "\n",
        "        self.lstm1 = nn.LSTMCell(input_size = 2048*3*1, hidden_size=128)\n",
        "        self.lstm2 = nn.LSTMCell(input_size = 2048*3*1, hidden_size=128)\n",
        "\n",
        "        self.classifier = nn.Linear(128, 1)\n",
        "\n",
        "    # def forward(self, x):\n",
        "    #     # x: [B, 1, 370, 1226]\n",
        "    #     x = self.backbone(x)              # [B, 2048, 12, 39]\n",
        "    #     x = x.permute(0, 2, 3, 1)         # [B, 12, 39, 2048]\n",
        "    #     x = x.reshape(x.size(0), -1, 2048)  # [B, 468, 2048]\n",
        "    #     lstm_out, _ = self.lstm(x)        # [B, 468, hidden]\n",
        "    #     out = self.classifier(lstm_out[:, -1, :])  # финальный временной шаг\n",
        "    #     return out\n",
        "\n",
        "    def forward(self,image1,image2):\n",
        "        out_features1 = self.backbone(image1)\n",
        "        out_features2 = self.backbone(image2)\n",
        "\n",
        "        out_features1 = torch.nn.functional.adaptive_avg_pool2d(out_features1, (1, 3))  # [B, C, 3, 10]\n",
        "        out_features1 = out_features1.view(out_features1.size(0), -1)  # [B, C * 3 * 10]\n",
        "\n",
        "        out_features2 = torch.nn.functional.adaptive_avg_pool2d(out_features2, (1, 3))\n",
        "        out_features2 = out_features2.view(out_features2.size(0), -1)\n",
        "        batch_size = out_features1.size(0)\n",
        "\n",
        "        # Инициализируем скрытые состояния и состояния ячеек для lstm1\n",
        "        h_t1 = torch.zeros(batch_size, self.lstm1.hidden_size, device=out_features1.device)\n",
        "        c_t1 = torch.zeros(batch_size, self.lstm1.hidden_size, device=out_features1.device)\n",
        "\n",
        "        # Пропускаем признаки первой картинки через lstm1 (один временной шаг)\n",
        "        h_t1_next, c_t1_next = self.lstm1(out_features1, (h_t1, c_t1))\n",
        "        h_t2_next, c_t2_next = self.lstm2(out_features2, (h_t1_next, c_t1_next))\n",
        "        out = self.classifier(h_t2_next) # [batch_size, 1]\n",
        "        return out\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr = 0.0001)\n",
        "\n",
        "'''"
      ],
      "metadata": {
        "id": "-PFsUtvsJYy5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Создание библиотеки для хранения сохранения"
      ],
      "metadata": {
        "id": "ir_n43vuJaVI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "directory_path = '/content/drive/My Drive/checpoint/model_resnet-50_lstm/'\n",
        "\n",
        "# Проверка существования директории\n",
        "if not os.path.exists(directory_path):\n",
        "    # Создание директории\n",
        "    os.makedirs(directory_path)\n",
        "    print(f\"Директория {directory_path} была создана.\")\n",
        "else:\n",
        "    print(f\"Директория {directory_path} уже существует.\")"
      ],
      "metadata": {
        "id": "tVB-P3TrJbii",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6b729db0-b31e-4b8a-f8e6-17e982aab954"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Директория /content/drive/My Drive/checpoint/model_resnet-50_lstm/ была создана.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Создание словаря для сохранения результатов"
      ],
      "metadata": {
        "id": "K-_OiYpyJeNm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "checkpoint_dict = {\n",
        "    'model_description' : txt_model_opt_shed,\n",
        "    'state_model' : None,\n",
        "    'state_opt'   : None,\n",
        "    'state_scheduler' : None,\n",
        "\n",
        "    'train_loss' :None,\n",
        "    'val_loss'   : None,\n",
        "    'best_loss'  : None,\n",
        "    'train_acc'  : None,\n",
        "    'val_acc'    : None,\n",
        "\n",
        "    'EPOCHS'     : None,\n",
        "    'current_epoch' : None,\n",
        "    'learning' : None\n",
        "}"
      ],
      "metadata": {
        "id": "vHDZ5Kb0Je4B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "MAIN"
      ],
      "metadata": {
        "id": "a8t-SugcJf95"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 8\n",
        "RANDOM_SEED = 123\n",
        "NUM_EPOCHS = 20\n",
        "last_epoch = 0\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"Використовується пристрій: {DEVICE}\")"
      ],
      "metadata": {
        "id": "rwRoiJbpJhB3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "da55454e-9261-4eed-a4c6-a496b7d7abf1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Використовується пристрій: cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "directory = '/content/drive/MyDrive'\n",
        "\n",
        "# Получение списка всех файлов и папок в директории\n",
        "files_and_folders = os.listdir(directory)\n",
        "\n",
        "# Отображаем только папки\n",
        "folders = [f for f in files_and_folders if os.path.isdir(os.path.join(directory, f))]\n",
        "\n",
        "print(folders)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-erQlTtYf3e4",
        "outputId": "feaf3166-cf49-45c0-be3a-6c15bad42c20"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Classroom', 'Programming', 'asd_lab_2-2', 'ТПР', '00', '01', '02', '03', '04', '05', '08', '09', '06', '07', '10', 'checpoint']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.optim.lr_scheduler import StepLR\n",
        "set_all_seeds(RANDOM_SEED)\n",
        "\n",
        "model = Network()\n",
        "model = model.to(DEVICE)\n",
        "# optimizer = torch.optim.Adam(model.parameters(), lr = 0.0001, weight_decay = 0.0006)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr = 0.0001)\n",
        "train_loader, validation_loader, test_loader = get_data(batch_size = BATCH_SIZE)\n",
        "# scheduler = StepLR(optimizer, step_size = 4, gamma = 0.3)\n",
        "scheduler = None\n",
        "scaler = GradScaler()\n",
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "print(f'The model has {count_parameters(model):,} trainable parameters')"
      ],
      "metadata": {
        "id": "2IkbrudJJiPW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3171ca90-6330-404d-9fd9-976f6cd83d98"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The model has 29,926,465 trainable parameters\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "<ipython-input-20-2abebc2f4c74>:11: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = GradScaler()\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Загрузка словаря из файла\n",
        "path = './2_epoch.pth'\n",
        "checkpoint = torch.load(path)\n",
        "if scheduler == None:\n",
        "    scheduler = StepLR(optimizer, step_size = 1, gamma = 1)\n",
        "# # Загрузка состояния модели\n",
        "# save_check = checkpoint['optimizer_state_dict']\n",
        "# save_check['lr'] = 0.0003\n",
        "model.load_state_dict( checkpoint['state_model'] )\n",
        "optimizer.load_state_dict( checkpoint['state_opt'] )\n",
        "scheduler.load_state_dict( checkpoint['state_scheduler'] )\n",
        "last_epoch = checkpoint['current_epoch']\n",
        "# # Загрузка состояния оптимизатора\n",
        "# optimizer.load_state_dict(save_check)"
      ],
      "metadata": {
        "id": "ITnSX-_tJkFn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if scheduler == None:\n",
        "    scheduler = StepLR(optimizer, step_size = 1, gamma = 1)\n",
        "minibatch_loss, train_acc, valid_acc = train(model = model,\n",
        "            num_epochs = NUM_EPOCHS,\n",
        "            last_epoch = last_epoch,\n",
        "            train_loader = train_loader,\n",
        "            validation_loader = validation_loader,\n",
        "            test_loader = test_loader,\n",
        "            optimizer = optimizer,\n",
        "            device = DEVICE,\n",
        "            scheduler = scheduler,\n",
        "            checkpoint_dict = checkpoint_dict,\n",
        "            scaler = scaler)"
      ],
      "metadata": {
        "id": "6qmlbAL0Jknw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2ef7d4b4-4360-4617-d523-26d65ab82bda"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-7-68ebc24c23c6>:17: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast():\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 001/020 | Batch 0000/2550 | Loss: 0.7133| Learning rate: 0.0001\n",
            "Epoch: 001/020 | Batch 0100/2550 | Loss: 0.0466| Learning rate: 0.0001\n",
            "Epoch: 001/020 | Batch 0200/2550 | Loss: 0.0218| Learning rate: 0.0001\n",
            "Epoch: 001/020 | Batch 0300/2550 | Loss: 0.1025| Learning rate: 0.0001\n",
            "Epoch: 001/020 | Batch 0400/2550 | Loss: 0.0304| Learning rate: 0.0001\n",
            "Epoch: 001/020 | Batch 0500/2550 | Loss: 0.0165| Learning rate: 0.0001\n",
            "Epoch: 001/020 | Batch 0600/2550 | Loss: 0.0125| Learning rate: 0.0001\n",
            "Epoch: 001/020 | Batch 0700/2550 | Loss: 0.0114| Learning rate: 0.0001\n",
            "Epoch: 001/020 | Batch 0800/2550 | Loss: 0.0598| Learning rate: 0.0001\n",
            "Epoch: 001/020 | Batch 0900/2550 | Loss: 0.0210| Learning rate: 0.0001\n",
            "Epoch: 001/020 | Batch 1000/2550 | Loss: 0.0101| Learning rate: 0.0001\n",
            "Epoch: 001/020 | Batch 1100/2550 | Loss: 0.0132| Learning rate: 0.0001\n",
            "Epoch: 001/020 | Batch 1200/2550 | Loss: 0.0055| Learning rate: 0.0001\n",
            "Epoch: 001/020 | Batch 1300/2550 | Loss: 0.0179| Learning rate: 0.0001\n",
            "Epoch: 001/020 | Batch 1400/2550 | Loss: 0.0048| Learning rate: 0.0001\n",
            "Epoch: 001/020 | Batch 1500/2550 | Loss: 0.0028| Learning rate: 0.0001\n",
            "Epoch: 001/020 | Batch 1600/2550 | Loss: 0.0036| Learning rate: 0.0001\n",
            "Epoch: 001/020 | Batch 1700/2550 | Loss: 0.0025| Learning rate: 0.0001\n",
            "Epoch: 001/020 | Batch 1800/2550 | Loss: 0.0040| Learning rate: 0.0001\n",
            "Epoch: 001/020 | Batch 1900/2550 | Loss: 0.0028| Learning rate: 0.0001\n",
            "Epoch: 001/020 | Batch 2000/2550 | Loss: 0.0053| Learning rate: 0.0001\n",
            "Epoch: 001/020 | Batch 2100/2550 | Loss: 0.0017| Learning rate: 0.0001\n",
            "Epoch: 001/020 | Batch 2200/2550 | Loss: 0.0094| Learning rate: 0.0001\n",
            "Epoch: 001/020 | Batch 2300/2550 | Loss: 0.0087| Learning rate: 0.0001\n",
            "Epoch: 001/020 | Batch 2400/2550 | Loss: 0.0054| Learning rate: 0.0001\n",
            "Epoch: 001/020 | Batch 2500/2550 | Loss: 0.0057| Learning rate: 0.0001\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-4-d0dc02461e82>:22: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast():\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 001/020 | Train AVG LOSS:  0.0046 | Accuracy_train:  0.8754 \n",
            "| Validation AVG LOSS:  0.0571 | Accuracy_val:  0.3119 \n",
            "Time elapsed: 40.44 min\n",
            "Epoch: 002/020 | Batch 0000/2550 | Loss: 0.0027| Learning rate: 0.0001\n",
            "Epoch: 002/020 | Batch 0100/2550 | Loss: 0.0051| Learning rate: 0.0001\n",
            "Epoch: 002/020 | Batch 0200/2550 | Loss: 0.0037| Learning rate: 0.0001\n",
            "Epoch: 002/020 | Batch 0300/2550 | Loss: 0.0040| Learning rate: 0.0001\n",
            "Epoch: 002/020 | Batch 0400/2550 | Loss: 0.0011| Learning rate: 0.0001\n",
            "Epoch: 002/020 | Batch 0500/2550 | Loss: 0.0025| Learning rate: 0.0001\n",
            "Epoch: 002/020 | Batch 0600/2550 | Loss: 0.0090| Learning rate: 0.0001\n",
            "Epoch: 002/020 | Batch 0700/2550 | Loss: 0.0028| Learning rate: 0.0001\n",
            "Epoch: 002/020 | Batch 0800/2550 | Loss: 0.0044| Learning rate: 0.0001\n",
            "Epoch: 002/020 | Batch 0900/2550 | Loss: 0.0027| Learning rate: 0.0001\n",
            "Epoch: 002/020 | Batch 1000/2550 | Loss: 0.0018| Learning rate: 0.0001\n",
            "Epoch: 002/020 | Batch 1100/2550 | Loss: 0.0044| Learning rate: 0.0001\n",
            "Epoch: 002/020 | Batch 1200/2550 | Loss: 0.0016| Learning rate: 0.0001\n",
            "Epoch: 002/020 | Batch 1300/2550 | Loss: 0.0016| Learning rate: 0.0001\n",
            "Epoch: 002/020 | Batch 1400/2550 | Loss: 0.0032| Learning rate: 0.0001\n",
            "Epoch: 002/020 | Batch 1500/2550 | Loss: 0.0015| Learning rate: 0.0001\n",
            "Epoch: 002/020 | Batch 1600/2550 | Loss: 0.0011| Learning rate: 0.0001\n",
            "Epoch: 002/020 | Batch 1700/2550 | Loss: 0.0011| Learning rate: 0.0001\n",
            "Epoch: 002/020 | Batch 1800/2550 | Loss: 0.0009| Learning rate: 0.0001\n",
            "Epoch: 002/020 | Batch 1900/2550 | Loss: 0.0049| Learning rate: 0.0001\n",
            "Epoch: 002/020 | Batch 2000/2550 | Loss: 0.0010| Learning rate: 0.0001\n",
            "Epoch: 002/020 | Batch 2100/2550 | Loss: 0.0019| Learning rate: 0.0001\n",
            "Epoch: 002/020 | Batch 2200/2550 | Loss: 0.0028| Learning rate: 0.0001\n",
            "Epoch: 002/020 | Batch 2300/2550 | Loss: 0.0060| Learning rate: 0.0001\n",
            "Epoch: 002/020 | Batch 2400/2550 | Loss: 0.0070| Learning rate: 0.0001\n",
            "Epoch: 002/020 | Batch 2500/2550 | Loss: 0.0018| Learning rate: 0.0001\n",
            "Epoch: 002/020 | Train AVG LOSS:  0.0032 | Accuracy_train:  0.9263 \n",
            "| Validation AVG LOSS:  0.0523 | Accuracy_val:  0.3604 \n",
            "Time elapsed: 77.71 min\n",
            "Epoch: 003/020 | Batch 0000/2550 | Loss: 0.0058| Learning rate: 0.0001\n",
            "Epoch: 003/020 | Batch 0100/2550 | Loss: 0.0007| Learning rate: 0.0001\n",
            "Epoch: 003/020 | Batch 0200/2550 | Loss: 0.0016| Learning rate: 0.0001\n",
            "Epoch: 003/020 | Batch 0300/2550 | Loss: 0.0013| Learning rate: 0.0001\n",
            "Epoch: 003/020 | Batch 0400/2550 | Loss: 0.0007| Learning rate: 0.0001\n",
            "Epoch: 003/020 | Batch 0500/2550 | Loss: 0.0053| Learning rate: 0.0001\n",
            "Epoch: 003/020 | Batch 0600/2550 | Loss: 0.0014| Learning rate: 0.0001\n",
            "Epoch: 003/020 | Batch 0700/2550 | Loss: 0.0038| Learning rate: 0.0001\n",
            "Epoch: 003/020 | Batch 0800/2550 | Loss: 0.0025| Learning rate: 0.0001\n",
            "Epoch: 003/020 | Batch 0900/2550 | Loss: 0.0010| Learning rate: 0.0001\n",
            "Epoch: 003/020 | Batch 1000/2550 | Loss: 0.0010| Learning rate: 0.0001\n",
            "Epoch: 003/020 | Batch 1100/2550 | Loss: 0.0016| Learning rate: 0.0001\n",
            "Epoch: 003/020 | Batch 1200/2550 | Loss: 0.0029| Learning rate: 0.0001\n",
            "Epoch: 003/020 | Batch 1300/2550 | Loss: 0.0033| Learning rate: 0.0001\n",
            "Epoch: 003/020 | Batch 1400/2550 | Loss: 0.0022| Learning rate: 0.0001\n",
            "Epoch: 003/020 | Batch 1500/2550 | Loss: 0.0045| Learning rate: 0.0001\n",
            "Epoch: 003/020 | Batch 1600/2550 | Loss: 0.0031| Learning rate: 0.0001\n",
            "Epoch: 003/020 | Batch 1700/2550 | Loss: 0.0020| Learning rate: 0.0001\n",
            "Epoch: 003/020 | Batch 1800/2550 | Loss: 0.0024| Learning rate: 0.0001\n",
            "Epoch: 003/020 | Batch 1900/2550 | Loss: 0.0036| Learning rate: 0.0001\n",
            "Epoch: 003/020 | Batch 2000/2550 | Loss: 0.0031| Learning rate: 0.0001\n",
            "Epoch: 003/020 | Batch 2100/2550 | Loss: 0.0018| Learning rate: 0.0001\n",
            "Epoch: 003/020 | Batch 2200/2550 | Loss: 0.0255| Learning rate: 0.0001\n",
            "Epoch: 003/020 | Batch 2300/2550 | Loss: 0.0010| Learning rate: 0.0001\n",
            "Epoch: 003/020 | Batch 2400/2550 | Loss: 0.0016| Learning rate: 0.0001\n",
            "Epoch: 003/020 | Batch 2500/2550 | Loss: 0.0026| Learning rate: 0.0001\n",
            "Epoch: 003/020 | Train AVG LOSS:  0.0029 | Accuracy_train:  0.9372 \n",
            "| Validation AVG LOSS:  0.0635 | Accuracy_val:  0.3176 \n",
            "Time elapsed: 114.96 min\n",
            "Epoch: 004/020 | Batch 0000/2550 | Loss: 0.0014| Learning rate: 0.0001\n",
            "Epoch: 004/020 | Batch 0100/2550 | Loss: 0.0017| Learning rate: 0.0001\n",
            "Epoch: 004/020 | Batch 0200/2550 | Loss: 0.0023| Learning rate: 0.0001\n",
            "Epoch: 004/020 | Batch 0300/2550 | Loss: 0.0012| Learning rate: 0.0001\n",
            "Epoch: 004/020 | Batch 0400/2550 | Loss: 0.0010| Learning rate: 0.0001\n",
            "Epoch: 004/020 | Batch 0500/2550 | Loss: 0.0022| Learning rate: 0.0001\n",
            "Epoch: 004/020 | Batch 0600/2550 | Loss: 0.0015| Learning rate: 0.0001\n",
            "Epoch: 004/020 | Batch 0700/2550 | Loss: 0.0006| Learning rate: 0.0001\n",
            "Epoch: 004/020 | Batch 0800/2550 | Loss: 0.0007| Learning rate: 0.0001\n",
            "Epoch: 004/020 | Batch 0900/2550 | Loss: 0.0012| Learning rate: 0.0001\n",
            "Epoch: 004/020 | Batch 1000/2550 | Loss: 0.0007| Learning rate: 0.0001\n",
            "Epoch: 004/020 | Batch 1100/2550 | Loss: 0.0013| Learning rate: 0.0001\n",
            "Epoch: 004/020 | Batch 1200/2550 | Loss: 0.0031| Learning rate: 0.0001\n",
            "Epoch: 004/020 | Batch 1300/2550 | Loss: 0.0003| Learning rate: 0.0001\n",
            "Epoch: 004/020 | Batch 1400/2550 | Loss: 0.0007| Learning rate: 0.0001\n",
            "Epoch: 004/020 | Batch 1500/2550 | Loss: 0.0039| Learning rate: 0.0001\n",
            "Epoch: 004/020 | Batch 1600/2550 | Loss: 0.0039| Learning rate: 0.0001\n",
            "Epoch: 004/020 | Batch 1700/2550 | Loss: 0.0021| Learning rate: 0.0001\n",
            "Epoch: 004/020 | Batch 1800/2550 | Loss: 0.0046| Learning rate: 0.0001\n",
            "Epoch: 004/020 | Batch 1900/2550 | Loss: 0.0014| Learning rate: 0.0001\n",
            "Epoch: 004/020 | Batch 2000/2550 | Loss: 0.0006| Learning rate: 0.0001\n",
            "Epoch: 004/020 | Batch 2100/2550 | Loss: 0.0012| Learning rate: 0.0001\n",
            "Epoch: 004/020 | Batch 2200/2550 | Loss: 0.0009| Learning rate: 0.0001\n",
            "Epoch: 004/020 | Batch 2300/2550 | Loss: 0.0007| Learning rate: 0.0001\n",
            "Epoch: 004/020 | Batch 2400/2550 | Loss: 0.0018| Learning rate: 0.0001\n",
            "Epoch: 004/020 | Batch 2500/2550 | Loss: 0.0018| Learning rate: 0.0001\n",
            "Epoch: 004/020 | Train AVG LOSS:  0.0026 | Accuracy_train:  0.9504 \n",
            "| Validation AVG LOSS:  0.0561 | Accuracy_val:  0.3094 \n",
            "Time elapsed: 152.25 min\n",
            "Epoch: 005/020 | Batch 0000/2550 | Loss: 0.0025| Learning rate: 0.0001\n",
            "Epoch: 005/020 | Batch 0100/2550 | Loss: 0.0024| Learning rate: 0.0001\n",
            "Epoch: 005/020 | Batch 0200/2550 | Loss: 0.0006| Learning rate: 0.0001\n",
            "Epoch: 005/020 | Batch 0300/2550 | Loss: 0.0015| Learning rate: 0.0001\n",
            "Epoch: 005/020 | Batch 0400/2550 | Loss: 0.0017| Learning rate: 0.0001\n",
            "Epoch: 005/020 | Batch 0500/2550 | Loss: 0.0020| Learning rate: 0.0001\n",
            "Epoch: 005/020 | Batch 0600/2550 | Loss: 0.0009| Learning rate: 0.0001\n",
            "Epoch: 005/020 | Batch 0700/2550 | Loss: 0.0033| Learning rate: 0.0001\n",
            "Epoch: 005/020 | Batch 0800/2550 | Loss: 0.0013| Learning rate: 0.0001\n",
            "Epoch: 005/020 | Batch 0900/2550 | Loss: 0.0012| Learning rate: 0.0001\n",
            "Epoch: 005/020 | Batch 1000/2550 | Loss: 0.0032| Learning rate: 0.0001\n",
            "Epoch: 005/020 | Batch 1100/2550 | Loss: 0.0012| Learning rate: 0.0001\n",
            "Epoch: 005/020 | Batch 1200/2550 | Loss: 0.0017| Learning rate: 0.0001\n",
            "Epoch: 005/020 | Batch 1300/2550 | Loss: 0.0018| Learning rate: 0.0001\n",
            "Epoch: 005/020 | Batch 1400/2550 | Loss: 0.0012| Learning rate: 0.0001\n",
            "Epoch: 005/020 | Batch 1500/2550 | Loss: 0.0011| Learning rate: 0.0001\n",
            "Epoch: 005/020 | Batch 1600/2550 | Loss: 0.0040| Learning rate: 0.0001\n",
            "Epoch: 005/020 | Batch 1700/2550 | Loss: 0.0007| Learning rate: 0.0001\n",
            "Epoch: 005/020 | Batch 1800/2550 | Loss: 0.0013| Learning rate: 0.0001\n",
            "Epoch: 005/020 | Batch 1900/2550 | Loss: 0.0019| Learning rate: 0.0001\n",
            "Epoch: 005/020 | Batch 2000/2550 | Loss: 0.0006| Learning rate: 0.0001\n",
            "Epoch: 005/020 | Batch 2100/2550 | Loss: 0.0014| Learning rate: 0.0001\n",
            "Epoch: 005/020 | Batch 2200/2550 | Loss: 0.0006| Learning rate: 0.0001\n",
            "Epoch: 005/020 | Batch 2300/2550 | Loss: 0.0014| Learning rate: 0.0001\n",
            "Epoch: 005/020 | Batch 2400/2550 | Loss: 0.0009| Learning rate: 0.0001\n",
            "Epoch: 005/020 | Batch 2500/2550 | Loss: 0.0007| Learning rate: 0.0001\n",
            "Epoch: 005/020 | Train AVG LOSS:  0.0012 | Accuracy_train:  0.9877 \n",
            "| Validation AVG LOSS:  0.0502 | Accuracy_val:  0.3623 \n",
            "Time elapsed: 189.60 min\n",
            "Epoch: 006/020 | Batch 0000/2550 | Loss: 0.0007| Learning rate: 0.0001\n",
            "Epoch: 006/020 | Batch 0100/2550 | Loss: 0.0006| Learning rate: 0.0001\n",
            "Epoch: 006/020 | Batch 0200/2550 | Loss: 0.0023| Learning rate: 0.0001\n",
            "Epoch: 006/020 | Batch 0300/2550 | Loss: 0.0012| Learning rate: 0.0001\n",
            "Epoch: 006/020 | Batch 0400/2550 | Loss: 0.0011| Learning rate: 0.0001\n",
            "Epoch: 006/020 | Batch 0500/2550 | Loss: 0.0006| Learning rate: 0.0001\n",
            "Epoch: 006/020 | Batch 0600/2550 | Loss: 0.0005| Learning rate: 0.0001\n",
            "Epoch: 006/020 | Batch 0700/2550 | Loss: 0.0008| Learning rate: 0.0001\n"
          ]
        }
      ]
    }
  ]
}
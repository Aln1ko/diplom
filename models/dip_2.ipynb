{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Открытие гугл диска"
      ],
      "metadata": {
        "id": "WnJ_hNVjO6L_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # !rm -rf /root/.config/Google/DriveFS\n",
        "# !rm -rf /root/.config/drive\n"
      ],
      "metadata": {
        "id": "JXAFZR3zPEQZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H-TPoApTOzv5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dd5d6bce-84f4-4aef-9f2f-fdffcd115504"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import os.path\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import random\n",
        "from torchvision import transforms"
      ],
      "metadata": {
        "id": "so_EZkw3PguU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Дата лоадер\n"
      ],
      "metadata": {
        "id": "C7G0lsapU2Em"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# def get_data(batch_size,image_folder= '/content/drive/MyDrive/00/image_0', movements_file = '/content/drive/MyDrive/00/00.txt'):\n",
        "#     # image_folder = '/content/drive/MyDrive/00/image_0' # замените на путь к вашей папке\n",
        "\n",
        "#     def get_images(image_folder):\n",
        "#         image_files = [os.path.join(image_folder, file) for file in os.listdir(image_folder) if file.endswith('.png')]\n",
        "#         image_files.sort()\n",
        "\n",
        "#         images = []\n",
        "#         for file_path in image_files:\n",
        "#             try:\n",
        "#                 img = Image.open(file_path).convert('L')\n",
        "\n",
        "#                 img_array = np.array(img, dtype=np.float32) / 255.0  # Преобразуем изображение в массив numpy\n",
        "#                 images.append(img_array)\n",
        "\n",
        "#             except Exception as e:\n",
        "#                 print(f\"Ошибка при загрузке {file_path}: {e}\")\n",
        "\n",
        "#         images = np.array(images) # преобразуем в numpy массив\n",
        "#         print(images.shape) # проверяем размерность массива\n",
        "#         return images\n",
        "\n",
        "#     images = get_images(image_folder)\n",
        "\n",
        "#     def create_movements(movements_file):\n",
        "#         movements = []\n",
        "#         with open(movements_file, 'r') as f:\n",
        "#             for line in f:\n",
        "#                 data = line.strip().split()\n",
        "#                 movements.append([float(x) for x in data])\n",
        "#         return movements\n",
        "\n",
        "#     movements = create_movements(movements_file)\n",
        "\n",
        "\n",
        "#     def transform_mov(movements):\n",
        "#         new_movements =  []\n",
        "#         for matrix in movements:\n",
        "#             new_matrix = [\n",
        "#                 matrix[0:4].tolist(),\n",
        "#                 matrix[4:8].tolist(),\n",
        "#                 matrix[8:12].tolist(),\n",
        "#                 [0,0,0,1]\n",
        "#                 ]\n",
        "#             new_matrix = torch.tensor(new_matrix)\n",
        "#             new_movements.append(new_matrix)\n",
        "\n",
        "#             res_movements = []\n",
        "\n",
        "#         for i in range(len(new_movements)-1):\n",
        "#             T1_inv = torch.linalg.inv(new_movements[i])\n",
        "#             T_rel = torch.matmul(new_movements[i+1], T1_inv)\n",
        "#             res_matrix = T_rel[:3]\n",
        "#             res_matrix = torch.cat([ res_matrix[0],  res_matrix[1],  res_matrix[2]])\n",
        "#             res_movements.append(res_matrix)\n",
        "\n",
        "#         return res_movements\n",
        "\n",
        "#     movements = transform_mov(movements)\n",
        "\n",
        "\n",
        "#     class ImageDataset(Dataset):\n",
        "#         def __init__(self,images, movements):\n",
        "#             self.images = images\n",
        "#             self.movements = movements\n",
        "\n",
        "#         def __len__(self):\n",
        "#             return len(self.movements)\n",
        "\n",
        "#         def __getitem__(self,idx):\n",
        "#             image1 = torch.tensor(self.images[idx],dtype = torch.float32).unsqueeze(0)\n",
        "#             image2 = torch.tensor(self.images[idx + 1],dtype = torch.float32).unsqueeze(0)\n",
        "#             movement = torch.tensor(self.movements[idx], dtype = torch.float32)\n",
        "#             return (image1,image2), movement\n",
        "\n",
        "#     train_size = int(0.9*len(images))\n",
        "#     train_dataset = ImageDataset(images[:train_size], movements[:train_size])\n",
        "#     validation_dataset = ImageDataset(images[train_size:], movements[train_size:])\n",
        "\n",
        "#     train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle = True)\n",
        "#     validation_loader = DataLoader(validation_dataset, batch_size=batch_size, shuffle = True)\n",
        "\n",
        "#     return train_loader,validation_loader\n"
      ],
      "metadata": {
        "id": "T6NPyYSjPBWX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_data(batch_size, image_folder='/content/drive/MyDrive/00/image_0', movements_file='/content/drive/MyDrive/00/00.txt'):\n",
        "    # Чтение движений из файла\n",
        "    def create_movements(movements_file):\n",
        "        movements = []\n",
        "        with open(movements_file, 'r') as f:\n",
        "            for line in f:\n",
        "                data = list(map(float, line.strip().split()))\n",
        "                movements.append(data)\n",
        "        return movements\n",
        "\n",
        "    movements = create_movements(movements_file)\n",
        "\n",
        "    # Преобразование движений\n",
        "    # def transform_mov(movements):\n",
        "    #     new_movements = []\n",
        "    #     for matrix in movements:\n",
        "    #         new_matrix = [\n",
        "    #             matrix[0:4],\n",
        "    #             matrix[4:8],\n",
        "    #             matrix[8:12],\n",
        "    #             [0, 0, 0, 1]\n",
        "    #         ]\n",
        "    #         new_matrix = torch.tensor(new_matrix, dtype=torch.float32)\n",
        "    #         new_movements.append(new_matrix)\n",
        "\n",
        "    #     res_movements = []\n",
        "    #     for i in range(len(new_movements) - 1):\n",
        "    #         T1_inv = torch.linalg.inv(new_movements[i])\n",
        "    #         T_rel = torch.matmul(new_movements[i + 1], T1_inv)\n",
        "    #         res_matrix = T_rel[:3]\n",
        "    #         res_matrix = torch.cat([res_matrix[0], res_matrix[1], res_matrix[2]])\n",
        "    #         res_movements.append(res_matrix)\n",
        "\n",
        "\n",
        "    #     return res_movements\n",
        "\n",
        "    def transform_mov(movements):\n",
        "        res_movements = []\n",
        "        for i in range(len(movements)-1):\n",
        "            m1 = torch.tensor(movements[i])\n",
        "            m2 = torch.tensor(movements[i+1])\n",
        "            res = m2-m1\n",
        "            distance = math.sqrt(pow(res[3],2) + pow(res[7],2) + pow(res[11],2))\n",
        "            res_movements.append([distance ])\n",
        "        return res_movements\n",
        "\n",
        "    movements = transform_mov(movements)\n",
        "\n",
        "    # Оптимизированный класс Dataset\n",
        "    class ImageDataset(Dataset):\n",
        "        def __init__(self, image_folder, movements,file_num1,file_num2, transform=None ):\n",
        "            # Получаем список всех файлов .png и сортируем их\n",
        "            all_image_files = sorted([os.path.join(image_folder, file) for file in os.listdir(image_folder) if file.endswith('.png')])\n",
        "\n",
        "            # Ограничиваем количество файлов, если передан file_num\n",
        "            if file_num1 and file_num2:\n",
        "                all_image_files = all_image_files[file_num1:file_num2]\n",
        "\n",
        "            self.image_files = all_image_files\n",
        "            self.movements = movements\n",
        "            self.transform = transform\n",
        "\n",
        "        def __len__(self):\n",
        "            # Возвращаем минимальную длину между движениями и изображениями - 1\n",
        "            return min(len(self.movements), len(self.image_files) - 1)\n",
        "\n",
        "\n",
        "        def __getitem__(self, idx):\n",
        "            image1_path = self.image_files[idx]\n",
        "            image2_path = self.image_files[idx + 1]\n",
        "\n",
        "            image1 = Image.open(image1_path).convert('L')\n",
        "            image2 = Image.open(image2_path).convert('L')\n",
        "\n",
        "            if self.transform:\n",
        "                image1 = self.transform(image1)\n",
        "                image2 = self.transform(image2)\n",
        "\n",
        "            movement = torch.tensor(self.movements[idx], dtype=torch.float32)\n",
        "            return (image1, image2), movement\n",
        "\n",
        "    # Трансформации для уменьшения использования памяти\n",
        "    transform = transforms.Compose([\n",
        "        transforms.Grayscale(),\n",
        "        transforms.ToTensor()\n",
        "    ])\n",
        "\n",
        "    # Разделение на тренировочную и валидационную выборки\n",
        "    train_size = int(0.9 * len(movements))\n",
        "    train_size = 4000\n",
        "    train_dataset = ImageDataset(image_folder, movements[:train_size], 0, train_size, transform=transform)\n",
        "    validation_dataset = ImageDataset(image_folder, movements[train_size:],train_size, len(movements), transform=transform)\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "    validation_loader = DataLoader(validation_dataset, batch_size=batch_size, shuffle=False)\n",
        "    test_loader = None\n",
        "    return train_loader, validation_loader,test_loader\n"
      ],
      "metadata": {
        "id": "fhxniKAhwSbJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import math"
      ],
      "metadata": {
        "id": "mbudv8Uh7DTv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_pathes(base_folder ):\n",
        "    paths = []\n",
        "    for i in range(11):\n",
        "        folder_name = f'{i:02d}'\n",
        "        image_folder = os.path.join(base_folder,folder_name,'image_0')\n",
        "        movements_file = os.path.join(base_folder,folder_name,f'{folder_name}.txt')\n",
        "        paths.append((image_folder,movements_file))\n",
        "    return paths\n",
        "\n",
        "def get_data(batch_size, base_folder = '/content/drive/MyDrive'):\n",
        "    paths = get_pathes(base_folder)\n",
        "\n",
        "    # Выбор папок для тренировочных, валидационных и тестовых данных\n",
        "    validation_folder = paths[-2]  # Предпоследняя папка\n",
        "    test_folder = paths[-1]        # Последняя папка\n",
        "    train_folders = paths[:-2]     # Все остальные папки\n",
        "\n",
        "    # Чтение движений из файла\n",
        "    def create_movements(movements_file):\n",
        "        movements = []\n",
        "        with open(movements_file, 'r') as f:\n",
        "            for line in f:\n",
        "                data = list(map(float, line.strip().split()))\n",
        "                movements.append(data)\n",
        "        return movements\n",
        "\n",
        "    # movements = create_movements(movements_file)\n",
        "\n",
        "    # Преобразование движений\n",
        "    # def transform_mov(movements):\n",
        "    #     new_movements = []\n",
        "    #     for matrix in movements:\n",
        "    #         new_matrix = [\n",
        "    #             matrix[0:4],\n",
        "    #             matrix[4:8],\n",
        "    #             matrix[8:12],\n",
        "    #             [0, 0, 0, 1]\n",
        "    #         ]\n",
        "    #         new_matrix = torch.tensor(new_matrix, dtype=torch.float32)\n",
        "    #         new_movements.append(new_matrix)\n",
        "\n",
        "    #     res_movements = []\n",
        "    #     for i in range(len(new_movements) - 1):\n",
        "    #         T1_inv = torch.linalg.inv(new_movements[i])\n",
        "    #         T_rel = torch.matmul(new_movements[i + 1], T1_inv)\n",
        "    #         res_matrix = T_rel[:3]\n",
        "    #         res_matrix = torch.cat([res_matrix[0], res_matrix[1], res_matrix[2]])\n",
        "    #         res_movements.append(res_matrix)\n",
        "\n",
        "\n",
        "    #     return res_movements\n",
        "\n",
        "    # def transform_mov(movements):\n",
        "    #     res_movements = []\n",
        "    #     for i in range(len(movements)-1):\n",
        "    #         m1 = torch.tensor(movements[i])\n",
        "    #         m2 = torch.tensor(movements[i+1])\n",
        "    #         res = m2-m1\n",
        "    #         res_movements.append(res)\n",
        "    #     return res_movements\n",
        "\n",
        "    def transform_mov(movements):\n",
        "        res_movements = []\n",
        "        for i in range(len(movements)-1):\n",
        "            m1 = torch.tensor(movements[i])\n",
        "            m2 = torch.tensor(movements[i+1])\n",
        "            res = m2-m1\n",
        "            distance = math.sqrt(pow(res[3],2) + pow(res[7],2) + pow(res[11],2))\n",
        "            res_movements.append([distance ])\n",
        "        return res_movements\n",
        "\n",
        "\n",
        "\n",
        "    # movements = transform_mov(movements)\n",
        "\n",
        "    # Оптимизированный класс Dataset\n",
        "    class ImageDataset(Dataset):\n",
        "        def __init__(self, image_folder, movements, transform=None ):\n",
        "            # Получаем список всех файлов .png и сортируем их\n",
        "            all_image_files = sorted([os.path.join(image_folder, file) for file in os.listdir(image_folder) if file.endswith('.png')])\n",
        "            self.image_files = all_image_files\n",
        "            self.movements = movements\n",
        "            self.transform = transform\n",
        "\n",
        "        def __len__(self):\n",
        "            # Возвращаем минимальную длину между движениями и изображениями - 1\n",
        "            return min(len(self.movements), len(self.image_files) - 1)\n",
        "\n",
        "\n",
        "        def __getitem__(self, idx):\n",
        "            image1_path = self.image_files[idx]\n",
        "            image2_path = self.image_files[idx + 1]\n",
        "\n",
        "            image1 = Image.open(image1_path).convert('L')\n",
        "            image2 = Image.open(image2_path).convert('L')\n",
        "\n",
        "            if self.transform:\n",
        "                image1 = self.transform(image1)\n",
        "                image2 = self.transform(image2)\n",
        "\n",
        "            movement = ( self.movements[idx].clone().detach().float()\n",
        "                        if isinstance(self.movements[idx], torch.Tensor)\n",
        "                        else torch.tensor(self.movements[idx], dtype=torch.float32) )\n",
        "            return (image1, image2), movement\n",
        "\n",
        "\n",
        "    # Трансформации для уменьшения использования памяти\n",
        "    transform = transforms.Compose([\n",
        "        transforms.Grayscale(),\n",
        "        transforms.Resize((376,1240)),\n",
        "        transforms.ToTensor()\n",
        "    ])\n",
        "\n",
        "    def load_dataset(folders):\n",
        "        images, movements = [], []\n",
        "        for image_folder, movements_file in folders:\n",
        "            mov = create_movements(movements_file)\n",
        "            mov = transform_mov(mov)\n",
        "            movements.append(mov)\n",
        "            images.append(image_folder)\n",
        "        return images, movements\n",
        "\n",
        "    train_images, train_movements = load_dataset(train_folders)\n",
        "    train_dataset = []\n",
        "    for index,image_folder in enumerate(train_images):\n",
        "        dataset = ImageDataset(image_folder, train_movements[index], transform=transform)\n",
        "        train_dataset.append(dataset)\n",
        "    train_dataset = torch.utils.data.ConcatDataset(train_dataset)\n",
        "\n",
        "    # Загрузка валидационных данных\n",
        "    val_images, val_movements = load_dataset([validation_folder])\n",
        "    val_dataset = ImageDataset(val_images[0], val_movements[0], transform=transform)\n",
        "\n",
        "\n",
        "   # Загрузка тестовых данных\n",
        "    test_images, test_movements = load_dataset([test_folder])\n",
        "    test_dataset = ImageDataset(test_images[0], test_movements[0], transform=transform)\n",
        "\n",
        "    # DataLoaders\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle = True, num_workers=4, pin_memory=True)\n",
        "    validation_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle = False, num_workers=4, pin_memory=True)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle = False, num_workers=4, pin_memory=True)\n",
        "\n",
        "    return train_loader, validation_loader, test_loader\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "NHQ6fhDJgfi-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train_loader, validation_loader = get_data(4)\n",
        "# print(type(train_loader))\n",
        "# (image1,image2),movements = next(iter(train_loader))\n",
        "# print(image1.shape,)\n",
        "# print(image2.shape)\n",
        "# print(movements.shape)\n",
        "# print(type(movements))\n",
        "# print(movements)\n",
        "# # print(train_loader.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0_DuRXZ5fxwm",
        "outputId": "e7545732-657a-4eb5-d65a-455d42598a10"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'torch.utils.data.dataloader.DataLoader'>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-11-9a000deb2846>:67: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  movement = torch.tensor(self.movements[idx], dtype=torch.float32)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([4, 1, 376, 1241])\n",
            "torch.Size([4, 1, 376, 1241])\n",
            "torch.Size([4, 12])\n",
            "<class 'torch.Tensor'>\n",
            "tensor([[ 9.9885e-01,  3.4306e-03, -4.7780e-02,  1.0794e+01, -3.3760e-03,\n",
            "          9.9999e-01,  1.2231e-03, -6.5439e-01,  4.7784e-02, -1.0604e-03,\n",
            "          9.9886e-01,  5.6170e+00],\n",
            "        [ 9.9991e-01, -8.3254e-03, -1.0746e-02,  5.6306e+00,  8.3262e-03,\n",
            "          9.9997e-01,  2.5423e-05, -1.2226e+00,  1.0745e-02, -1.1488e-04,\n",
            "          9.9994e-01, -1.2722e+00],\n",
            "        [ 9.9995e-01,  8.3675e-03, -4.9610e-03,  2.2081e+00, -8.3621e-03,\n",
            "          9.9996e-01,  1.1193e-03, -3.9281e-02,  4.9702e-03, -1.0778e-03,\n",
            "          9.9999e-01,  3.4265e-01],\n",
            "        [ 9.9999e-01, -4.4478e-03,  3.0920e-03, -1.7784e-01,  4.4483e-03,\n",
            "          9.9999e-01, -1.6031e-04, -2.7174e-01, -3.0909e-03,  1.7405e-04,\n",
            "          1.0000e+00,  8.1690e-01]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# movements_file='/content/drive/MyDrive/00/00.txt'\n",
        "# # Чтение движений из файла\n",
        "# def create_movements(movements_file):\n",
        "#     movements = []\n",
        "#     with open(movements_file, 'r') as f:\n",
        "#         for line in f:\n",
        "#             data = list(map(float, line.strip().split()))\n",
        "#             movements.append(data)\n",
        "#     return movements\n",
        "\n",
        "# movements = create_movements(movements_file)\n",
        "# print(\"start movement after opening and spliting \",movements[:2])\n",
        "\n",
        "# # Преобразование движений\n",
        "# def transform_mov(movements):\n",
        "#     new_movements = []\n",
        "#     for matrix in movements:\n",
        "#         new_matrix = [\n",
        "#             matrix[0:4],\n",
        "#             matrix[4:8],\n",
        "#             matrix[8:12],\n",
        "#             [0, 0, 0, 1]\n",
        "#         ]\n",
        "#         new_matrix = torch.tensor(new_matrix, dtype=torch.float32)\n",
        "#         new_movements.append(new_matrix)\n",
        "#     print(\"после преобразование в матрицу 4 на 4 и добавление одной строки \", new_movements[:2])\n",
        "\n",
        "#     res_movements = []\n",
        "#     # for i in range(len(new_movements) - 1):\n",
        "#     for i in range(2):\n",
        "#         T1_inv = torch.linalg.inv(new_movements[i])\n",
        "#         T_rel = torch.matmul(new_movements[i + 1], T1_inv)\n",
        "#         res_matrix = T_rel[:3]\n",
        "#         res_matrix = torch.cat([res_matrix[0], res_matrix[1], res_matrix[2]])\n",
        "#         print(\"после преобразование матриці \", res_matrix)\n",
        "#         res_movements.append(res_matrix)\n",
        "#         print(\"добавление в один массив \", res_movements)\n",
        "\n",
        "#     return res_movements\n",
        "\n",
        "# movements = transform_mov(movements)\n"
      ],
      "metadata": {
        "id": "j68Ci5PPcwVS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "EVALUATION нужно еще функцию написать для подсчета точности или потерь вот тут внизу"
      ],
      "metadata": {
        "id": "Fo84FkeNVB71"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "CB_fc4meVDlS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def set_all_seeds(seed):\n",
        "    os.environ[\"PL_GLOBAL_SEED\"] = str(seed)\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n"
      ],
      "metadata": {
        "id": "gwjk1V5AU5Nq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_accuracy(model, data_loader, device):\n",
        "    model.eval()\n",
        "    total_loss = 0.0\n",
        "    num_batches = 0\n",
        "    correct_predictions = 0   # Счетчик корректных предсказаний\n",
        "    total_samples = 0         # Общее количество элементов\n",
        "\n",
        "    criterion = torch.nn.MSELoss()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, ((feature1, feature2),targets) in enumerate(data_loader):\n",
        "            feature1 = feature1.to(device)\n",
        "            feature2 = feature2.to(device)\n",
        "            targets = targets.to(device)\n",
        "\n",
        "            predicted_y = model(feature1, feature2)\n",
        "\n",
        "            # Вычисление ошибки\n",
        "            loss = criterion(predicted_y, targets)\n",
        "            total_loss += loss.item()\n",
        "            num_batches += 1\n",
        "\n",
        "            # Проверка критерия качества\n",
        "            relative_difference = torch.abs((predicted_y - targets) / targets)  # Расчет относительного отклонения\n",
        "            valid_predictions = torch.all(relative_difference <= 0.1, dim=1)    # Условие: отклонение <= 10%\n",
        "            correct_predictions += torch.sum(valid_predictions).item()\n",
        "            total_samples = total_samples + targets.size(0)\n",
        "\n",
        "\n",
        "    avg_loss = total_loss / num_batches             # Расчет средней ошибки\n",
        "    accuracy = correct_predictions / total_samples  # Расчет доли верных предсказаний\n",
        "    return avg_loss, accuracy"
      ],
      "metadata": {
        "id": "Arxfxo96gj0N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model"
      ],
      "metadata": {
        "id": "m3xV_zOfVNwh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F"
      ],
      "metadata": {
        "id": "IVlFigslVM2S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "model 1"
      ],
      "metadata": {
        "id": "kl34qshckACA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Network(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        self.features = nn.Sequential(\n",
        "            nn.Conv2d(1, 32, kernel_size = 4, stride = 2),# 187x620\n",
        "            nn.BatchNorm2d(32),\n",
        "            nn.ReLU(),\n",
        "            # nn.MaxPool2d(2, 2),  # 93x310\n",
        "            nn.Dropout(0.25),\n",
        "\n",
        "            nn.Conv2d(32, 32, kernel_size = 4, stride = 2),# 93x110\n",
        "            nn.BatchNorm2d(32),\n",
        "            nn.ReLU(),\n",
        "            # nn.Dropout(0.25),\n",
        "\n",
        "            nn.Conv2d(32, 64, kernel_size = 4, stride = 2),#45x154\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(),\n",
        "            # nn.MaxPool2d(2, 2),  # 22x77\n",
        "            # nn.Dropout(0.25),\n",
        "\n",
        "            nn.Conv2d(64, 64, kernel_size = 4, stride = 2),# 93x110\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(),\n",
        "            # nn.Dropout(0.25),\n",
        "\n",
        "            nn.Conv2d(64, 128, kernel_size = 4, stride = 2),# 10x37\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.ReLU(),\n",
        "            # nn.MaxPool2d(2, 2),  # 5x18\n",
        "            # nn.Dropout(0.25),\n",
        "\n",
        "            nn.Conv2d(128, 128, kernel_size = 4, stride = 2),# 10x37\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.ReLU(),\n",
        "            # nn.Dropout(0.25),\n",
        "\n",
        "            # nn.Conv2d(128, 256, kernel_size = 4,step = 2),\n",
        "            # nn.BatchNorm2d(256),\n",
        "            # nn.ReLU(),\n",
        "            nn.AdaptiveAvgPool2d((5, 5))  # Глобальный пуллинг к фиксированному размеру\n",
        "        )\n",
        "        self.lstm1 = nn.LSTMCell(input_size=128 * 5 * 5, hidden_size=256)\n",
        "        self.lstm2 = nn.LSTMCell(input_size=128 * 5 * 5, hidden_size=256)\n",
        "\n",
        "        self.network = nn.Sequential(\n",
        "            # nn.Linear(2 * 128 * 5 * 5, 128),\n",
        "            # nn.ReLU(),\n",
        "            # nn.Dropout(p=0.5),\n",
        "            # nn.Linear(128, 64),\n",
        "            # nn.ReLU(),\n",
        "            # nn.Dropout(p=0.5),\n",
        "            # nn.Linear(64, 12),\n",
        "            nn.Linear(256, 1)\n",
        "        )\n",
        "        # self.network = nn.Linear(2*128*5*5,12)\n",
        "\n",
        "\n",
        "    def forward(self,image1,image2):\n",
        "        out_features1 = self.features(image1)\n",
        "        out_features2 = self.features(image2)\n",
        "\n",
        "        out_features1 = torch.flatten(out_features1, 1) # Flatten all dimensions except batch\n",
        "        out_features2 = torch.flatten(out_features2, 1)\n",
        "\n",
        "        batch_size = out_features1.size(0)\n",
        "\n",
        "        # Инициализируем скрытые состояния и состояния ячеек для lstm1\n",
        "        h_t1 = torch.zeros(batch_size, self.lstm1.hidden_size, device=out_features1.device)\n",
        "        c_t1 = torch.zeros(batch_size, self.lstm1.hidden_size, device=out_features1.device)\n",
        "\n",
        "        # Пропускаем признаки первой картинки через lstm1 (один временной шаг)\n",
        "        h_t1_next, c_t1_next = self.lstm1(out_features1, (h_t1, c_t1))\n",
        "\n",
        "        # Пропускаем признаки второй картинки через lstm2 (один временной шаг)\n",
        "        h_t2_next, c_t2_next = self.lstm2(out_features2, (h_t1_next, c_t1_next))\n",
        "\n",
        "        # Конкатенируем выходные скрытые состояния от обоих LSTMCell\n",
        "        # out = torch.cat((h_t1_next, h_t2_next), dim=1) # [batch_size, 256]\n",
        "\n",
        "        out = self.network(h_t2_next) # [batch_size, 1]\n",
        "        return out\n",
        "\n",
        "\n",
        "        # out_features = torch.cat((out_features1,out_features2),dim = 1 )\n",
        "        # out = self.network(out_features)\n",
        "        # return out"
      ],
      "metadata": {
        "id": "CtSWiz2FVT0K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Alternative Model 1\n"
      ],
      "metadata": {
        "id": "_p24JYYxMuqV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Network(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        self.features = nn.Sequential(\n",
        "            nn.Conv2d(2, 32, kernel_size = 5, stride = 2),# 187x620\n",
        "            nn.BatchNorm2d(32),\n",
        "            nn.ReLU(),\n",
        "            # nn.MaxPool2d(2, 2),  # 93x310\n",
        "            # nn.Dropout(0.25),\n",
        "\n",
        "            nn.Conv2d(32, 32, kernel_size= 4, stride= 2, groups= 32),# 93x310\n",
        "            nn.Conv2d(32, 32, kernel_size= 1),\n",
        "            nn.BatchNorm2d(32),\n",
        "            nn.ReLU(),\n",
        "            # nn.Dropout(0.25),\n",
        "\n",
        "            nn.Conv2d(32, 64, kernel_size = 4, stride = 2),#45x155\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(),\n",
        "            # nn.MaxPool2d(2, 2),  # 22x77\n",
        "            # nn.Dropout(0.25),\n",
        "\n",
        "            nn.Conv2d(64, 64, kernel_size= 4, stride= 2, groups= 64),# 22x77\n",
        "            nn.Conv2d(64, 64, kernel_size= 1),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(),\n",
        "            # nn.Dropout(0.25),\n",
        "\n",
        "            nn.Conv2d(64, 128, kernel_size = 4, stride = 2),# 11x38\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.ReLU(),\n",
        "            # nn.MaxPool2d(2, 2),  # 5x18\n",
        "            # nn.Dropout(0.25),\n",
        "\n",
        "            nn.Conv2d(128, 128, kernel_size = 4, stride = 2, groups= 128),# 5x19\n",
        "            nn.Conv2d(128, 128, kernel_size= 1),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.ReLU(),\n",
        "            # nn.Dropout(0.25),\n",
        "\n",
        "            # nn.Conv2d(128, 256, kernel_size = 4,step = 2),\n",
        "            # nn.BatchNorm2d(256),\n",
        "            # nn.ReLU(),\n",
        "            # nn.AdaptiveAvgPool2d((5, 5))  # Глобальный пуллинг к фиксированному размеру\n",
        "        )\n",
        "        # self.lstm1 = nn.LSTMCell(input_size=128 * 5 * 5, hidden_size=256)\n",
        "        # self.lstm2 = nn.LSTMCell(input_size=128 * 5 * 5, hidden_size=256)\n",
        "\n",
        "        self.network = nn.Sequential(\n",
        "            nn.Linear(6528, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, 1)\n",
        "        )\n",
        "        # self.network = nn.Linear(2*128*5*5,12)\n",
        "\n",
        "\n",
        "    def forward(self,image1,image2):\n",
        "        # print('__1')\n",
        "        images = torch.cat((image1, image2), dim=1)\n",
        "        # print('images shape  ', images.shape)\n",
        "        out_features = self.features(images)\n",
        "        # print('len out_features  ', out_features.shape)\n",
        "        out_features = torch.flatten(out_features, 1)\n",
        "        # print('float out_features  ', out_features.shape)\n",
        "        out = self.network(out_features)\n",
        "        # print('__3')\n",
        "        return out\n",
        "\n",
        "\n",
        "        # out_features = torch.cat((out_features1,out_features2),dim = 1 )\n",
        "        # out = self.network(out_features)\n",
        "        # return out"
      ],
      "metadata": {
        "id": "GA1sNz-JM6nk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "EfficientNetV2\n"
      ],
      "metadata": {
        "id": "8TLRKFjHc3a9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MBConvBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, expansion_factor, stride, kernel_size=3):\n",
        "        super(MBConvBlock, self).__init__()\n",
        "\n",
        "        # Слой расширения (expansion)\n",
        "        self.expand = nn.Conv2d(in_channels, in_channels * expansion_factor, kernel_size=1)\n",
        "        self.bn1 = nn.BatchNorm2d(in_channels * expansion_factor)\n",
        "\n",
        "        # Depthwise свертка\n",
        "        self.depthwise = nn.Conv2d(in_channels * expansion_factor, in_channels * expansion_factor,\n",
        "                                    kernel_size = kernel_size, stride = stride,\n",
        "                                   padding = kernel_size//2, groups = in_channels * expansion_factor)\n",
        "        self.bn2 = nn.BatchNorm2d(in_channels * expansion_factor)\n",
        "\n",
        "        # Сжатие (projection) и выходной слой\n",
        "        self.project = nn.Conv2d(in_channels * expansion_factor, out_channels, kernel_size=1)\n",
        "        self.bn3 = nn.BatchNorm2d(out_channels)\n",
        "\n",
        "        self.use_residual = (in_channels == out_channels) and (stride == 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        identity = x\n",
        "\n",
        "        x = F.relu(self.bn1(self.expand(x)))\n",
        "        x = F.relu(self.bn2(self.depthwise(x)))\n",
        "        x = self.bn3(self.project(x))\n",
        "\n",
        "        if self.use_residual:\n",
        "            x += identity\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "class Network(nn.Module):\n",
        "    def __init__(self, num_classes=1):\n",
        "        super(Network, self).__init__()\n",
        "\n",
        "        # Первый сверточный слой\n",
        "        self.conv_stem = nn.Conv2d(2, 32, kernel_size= (6, 10), stride= (2, 3), padding=1)\n",
        "        self.bn1 = nn.BatchNorm2d(32)\n",
        "\n",
        "        # Последовательность MBConv блоков\n",
        "        self.blocks = nn.Sequential(\n",
        "            MBConvBlock(32, 16, expansion_factor=1, stride=1),\n",
        "            MBConvBlock(16, 24, expansion_factor=6, stride=2),\n",
        "            MBConvBlock(24, 40, expansion_factor=6, stride=2),\n",
        "            MBConvBlock(40, 80, expansion_factor=6, stride=2),\n",
        "            MBConvBlock(80, 112, expansion_factor=6, stride=1),\n",
        "            MBConvBlock(112, 192, expansion_factor=6, stride=2),\n",
        "            MBConvBlock(192, 320, expansion_factor=6, stride=1)\n",
        "        )\n",
        "\n",
        "        # Глобальный Average Pooling\n",
        "        self.global_pool = nn.AdaptiveAvgPool2d(1)\n",
        "\n",
        "        # Полносвязный слой для классификации\n",
        "        self.fc = nn.Linear(320, num_classes)\n",
        "\n",
        "    def forward(self, image1,image2):\n",
        "        x = torch.cat((image1, image2), dim=1)\n",
        "        x = F.relu(self.bn1(self.conv_stem(x)))\n",
        "        x = self.blocks(x)\n",
        "        x = self.global_pool(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = self.fc(x)\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "HJL1tPlLc1-z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "EfficientNetV2  version 2\n"
      ],
      "metadata": {
        "id": "dTHr24nuZd8K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MBConvBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, expansion_factor, stride, kernel_size=3):\n",
        "        super(MBConvBlock, self).__init__()\n",
        "\n",
        "        self.use_residual = (in_channels == out_channels) and (stride == 1)\n",
        "        hidden_dim = in_channels * expansion_factor\n",
        "\n",
        "        self.conv = nn.Sequential(\n",
        "            # Слой расширения (expansion)\n",
        "            nn.Conv2d(in_channels, hidden_dim, kernel_size=1, bias=False),\n",
        "            nn.BatchNorm2d(hidden_dim),\n",
        "            nn.ReLU6(inplace=True),\n",
        "            # Depthwise свертка\n",
        "            nn.Conv2d(hidden_dim, hidden_dim, kernel_size=kernel_size, stride=stride,\n",
        "                      padding=kernel_size // 2, groups=hidden_dim, bias=False),\n",
        "            nn.BatchNorm2d(hidden_dim),\n",
        "            nn.ReLU6(inplace=True),\n",
        "            # Сжатие (projection)\n",
        "            nn.Conv2d(hidden_dim, out_channels, kernel_size=1, bias=False),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        if self.use_residual:\n",
        "            return x + self.conv(x)\n",
        "        else:\n",
        "            return self.conv(x)\n",
        "\n",
        "class Network(nn.Module):\n",
        "    def __init__(self, num_classes=1):\n",
        "        super(Network, self).__init__()\n",
        "\n",
        "         # Первый сверточный слой с измененными параметрами\n",
        "        self.conv_stem = nn.Conv2d(2, 32, kernel_size=(6, 10), stride=(2, 3), padding=1)# 187, 411\n",
        "        self.bn1 = nn.BatchNorm2d(32)\n",
        "        self.relu1 = nn.ReLU6(inplace=True)\n",
        "\n",
        "        # Последовательность MBConv блоков\n",
        "        self.blocks = nn.Sequential(\n",
        "            MBConvBlock(32, 16, expansion_factor=1, stride=1), #187, 411\n",
        "            MBConvBlock(16, 24, expansion_factor=6, stride=2), # 94, 206\n",
        "            MBConvBlock(24, 40, expansion_factor=6, stride=2),# 47, 103\n",
        "            MBConvBlock(40, 80, expansion_factor=6, stride=2),# 24, 52\n",
        "            MBConvBlock(80, 112, expansion_factor=6, stride=1),# 24, 52\n",
        "            MBConvBlock(112, 192, expansion_factor=6, stride=2),# 12, 26\n",
        "            MBConvBlock(192, 320, expansion_factor=6, stride=1) # 12, 26\n",
        "        )\n",
        "\n",
        "        # Последний сверточный слой\n",
        "        self.conv_head = nn.Conv2d(320, 1280, kernel_size=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(1280)\n",
        "        self.relu2 = nn.ReLU6(inplace=True)\n",
        "\n",
        "        # Глобальный Average Pooling\n",
        "        self.global_pool = nn.AdaptiveAvgPool2d(1)\n",
        "\n",
        "        # Полносвязный слой для классификации\n",
        "        self.fc = nn.Linear(1280, num_classes)\n",
        "\n",
        "        self._initialize_weights()\n",
        "\n",
        "    def forward(self, image1, image2):\n",
        "        x = torch.cat((image1, image2), dim=1)\n",
        "        x = self.relu1(self.bn1(self.conv_stem(x)))\n",
        "        x = self.blocks(x)\n",
        "        x = self.relu2(self.bn2(self.conv_head(x)))\n",
        "        x = self.global_pool(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = self.fc(x)\n",
        "        return x\n",
        "\n",
        "    def _initialize_weights(self):\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                nn.init.kaiming_normal_(m.weight, mode='fan_out')\n",
        "                if m.bias is not None:\n",
        "                    nn.init.zeros_(m.bias)\n",
        "            elif isinstance(m, nn.BatchNorm2d):\n",
        "                nn.init.ones_(m.weight)\n",
        "                nn.init.zeros_(m.bias)\n",
        "            elif isinstance(m, nn.Linear):\n",
        "                nn.init.normal_(m.weight, 0, 0.01)\n",
        "                nn.init.zeros_(m.bias)"
      ],
      "metadata": {
        "id": "Mig7IkZnZ0sr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "directory_path = '/content/drive/My Drive/checpoint/model_3_5/'\n",
        "\n",
        "# Проверка существования директории\n",
        "if not os.path.exists(directory_path):\n",
        "    # Создание директории\n",
        "    os.makedirs(directory_path)\n",
        "    print(f\"Директория {directory_path} была создана.\")\n",
        "else:\n",
        "    print(f\"Директория {directory_path} уже существует.\")"
      ],
      "metadata": {
        "id": "PC78LZ4HA0CI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cb63c678-5190-4b19-cba0-0fab4c9731d9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Директория /content/drive/My Drive/checpoint/model_3_5/ была создана.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Описание модели, оптимизатора, шедулера"
      ],
      "metadata": {
        "id": "nrcS9GtZmyLY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "txt_model_opt_shed = '''\n",
        "\n",
        "  model = Network()\n",
        "  model = model.to(DEVICE)\n",
        "  optimizer = torch.optim.Adam(model.parameters(),lr = 0.001, weight_decay=0.01)\n",
        "  train_loader, validation_loader,test_loader = get_data(batch_size = BATCH_SIZE)\n",
        "  scheduler = StepLR(optimizer, step_size=2, gamma=0.3)\n",
        "  def count_parameters(model):\n",
        "      return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "  print(f'The model has {count_parameters(model):,} trainable parameters')\n",
        "'''"
      ],
      "metadata": {
        "id": "QLMUMb_cnBtU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Создание словаря для сохранения результатов"
      ],
      "metadata": {
        "id": "OMFkqG81qWz7"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "oFF7Z3aVMaj2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "checkpoint_dict = {\n",
        "    'model_description' : txt_model_opt_shed,\n",
        "    'state_model' : None,\n",
        "    'state_opt'   : None,\n",
        "    'state_scheduler' : None,\n",
        "\n",
        "    'train_loss' :None,\n",
        "    'val_loss'   : None,\n",
        "    'best_loss'  : None,\n",
        "    'train_acc'  : None,\n",
        "    'val_acc'    : None,\n",
        "\n",
        "    'EPOCHS'     : None,\n",
        "    'current_epoch' : None,\n",
        "    'learning' : None\n",
        "\n",
        "}"
      ],
      "metadata": {
        "id": "KEur6K_Yqvg8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "EfficientNetV2 version 3"
      ],
      "metadata": {
        "id": "XnBP48VnmJHP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MBConvBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, expansion_factor, stride, kernel_size=3):\n",
        "        super(MBConvBlock, self).__init__()\n",
        "\n",
        "        self.use_residual = (in_channels == out_channels) and (stride == 1)\n",
        "        hidden_dim = in_channels * expansion_factor\n",
        "\n",
        "        self.conv = nn.Sequential(\n",
        "            # Слой расширения (expansion)\n",
        "            nn.Conv2d(in_channels, hidden_dim, kernel_size=1),\n",
        "            nn.BatchNorm2d(hidden_dim),\n",
        "            nn.ReLU6(inplace=True),\n",
        "\n",
        "            # Depthwise свертка\n",
        "            nn.Conv2d(hidden_dim, hidden_dim, kernel_size=kernel_size, stride=stride,\n",
        "                      padding=kernel_size // 2, groups=hidden_dim),\n",
        "            nn.BatchNorm2d(hidden_dim),\n",
        "            nn.ReLU6(inplace=True),\n",
        "            nn.Dropout2d(p= 0.2),\n",
        "\n",
        "            # Сжатие (projection)\n",
        "            nn.Conv2d(hidden_dim, out_channels, kernel_size=1),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        if self.use_residual:\n",
        "            return x + self.conv(x)\n",
        "            print(self.conv(x).shape)\n",
        "        else:\n",
        "            return self.conv(x)\n",
        "            print(self.conv(x).shape)\n",
        "\n",
        "class Network(nn.Module):\n",
        "    def __init__(self, num_classes=1):\n",
        "        super(Network, self).__init__()\n",
        "\n",
        "        # Первый сверточный слой с измененными параметрами\n",
        "        self.conv_stem = nn.Conv2d(2, 32, kernel_size=(6, 10), stride=(2, 3), padding=1)# 187, 411\n",
        "        self.bn1 = nn.BatchNorm2d(32)\n",
        "        self.relu1 = nn.ReLU6(inplace=True)\n",
        "\n",
        "        self.blocks = nn.Sequential(\n",
        "            # MBConvBlock(32, 64, expansion_factor=1, stride=2), #\n",
        "            MBConvBlock(32, 48, expansion_factor=6, stride=2),# 93, 206\n",
        "            MBConvBlock(48, 64, expansion_factor=6, stride=2),# 47, 103\n",
        "            MBConvBlock(64, 64, expansion_factor=6, stride=1), # 47, 103\n",
        "            MBConvBlock(64, 128, expansion_factor=6, stride=2),# 24, 52\n",
        "            MBConvBlock(128, 128, expansion_factor=6, stride=1),# 24, 52\n",
        "            MBConvBlock(128, 256, expansion_factor=6, stride=2),# 12, 26\n",
        "            MBConvBlock(256, 256, expansion_factor=6, stride=1),# 12, 26\n",
        "            MBConvBlock(256, 512, expansion_factor=6, stride=2), # 6, 13\n",
        "            MBConvBlock(512, 512, expansion_factor=6, stride=2) # 3, 6\n",
        "\n",
        "        )\n",
        "\n",
        "        # # Глобальный Average Pooling\n",
        "        # self.global_pool = nn.AdaptiveAvgPool2d(1)\n",
        "\n",
        "        # Полносвязный слой для классификации\n",
        "        self.fc1 = nn.Linear(512*3*7, 16)\n",
        "        self.relu_fc = nn.ReLU(inplace=True)\n",
        "        self.dropout_fc = nn.Dropout(p= 0.2) # Dropout после первого FC слоя\n",
        "        self.fc2 = nn.Linear(16, 1)\n",
        "\n",
        "        self._initialize_weights()\n",
        "\n",
        "    def forward(self, image1, image2):\n",
        "        x = torch.cat((image1, image2), dim=1)\n",
        "        x = self.relu1(self.bn1(self.conv_stem(x)))\n",
        "        x = self.blocks(x)\n",
        "        # x = self.relu2(self.bn2(self.conv_head(x)))\n",
        "        # x = self.global_pool(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = self.dropout_fc(self.relu_fc(self.fc1(x)))\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "    def _initialize_weights(self):\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                nn.init.kaiming_normal_(m.weight, mode='fan_out')\n",
        "                if m.bias is not None:\n",
        "                    nn.init.zeros_(m.bias)\n",
        "            elif isinstance(m, nn.BatchNorm2d):\n",
        "                nn.init.ones_(m.weight)\n",
        "                nn.init.zeros_(m.bias)\n",
        "            elif isinstance(m, nn.Linear):\n",
        "                nn.init.normal_(m.weight, 0, 0.01)\n",
        "                nn.init.zeros_(m.bias)"
      ],
      "metadata": {
        "id": "GyYUNP2gmFxz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "EfficientNetV2 version 4"
      ],
      "metadata": {
        "id": "5PPI6z4W0xad"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MBConvBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, expansion_factor, stride, kernel_size=3):\n",
        "        super(MBConvBlock, self).__init__()\n",
        "\n",
        "        self.use_residual = (in_channels == out_channels) and (stride == 1)\n",
        "        hidden_dim = in_channels * expansion_factor\n",
        "\n",
        "        self.conv = nn.Sequential(\n",
        "            # Слой расширения (expansion)\n",
        "            nn.Conv2d(in_channels, hidden_dim, kernel_size=1),\n",
        "            nn.BatchNorm2d(hidden_dim),\n",
        "            nn.ReLU6(inplace=True),\n",
        "\n",
        "            # Depthwise свертка\n",
        "            nn.Conv2d(hidden_dim, hidden_dim, kernel_size=kernel_size, stride=stride,\n",
        "                      padding=kernel_size // 2, groups=hidden_dim),\n",
        "            nn.BatchNorm2d(hidden_dim),\n",
        "            nn.ReLU6(inplace=True),\n",
        "\n",
        "            # Сжатие (projection)\n",
        "            nn.Conv2d(hidden_dim, out_channels, kernel_size=1),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        if self.use_residual:\n",
        "            return x + self.conv(x)\n",
        "            print(self.conv(x).shape)\n",
        "        else:\n",
        "            return self.conv(x)\n",
        "            print(self.conv(x).shape)\n",
        "\n",
        "class Network(nn.Module):\n",
        "    def __init__(self, num_classes=1):\n",
        "        super(Network, self).__init__()\n",
        "\n",
        "        # Первый сверточный слой с измененными параметрами\n",
        "        self.conv_stem = nn.Conv2d(2, 32, kernel_size=(6, 10), stride=(2, 3), padding=1)# 187, 411\n",
        "        self.bn1 = nn.BatchNorm2d(32)\n",
        "        self.relu1 = nn.ReLU6(inplace=True)\n",
        "\n",
        "        self.blocks = nn.Sequential(\n",
        "            # MBConvBlock(32, 64, expansion_factor=1, stride=2), #\n",
        "            MBConvBlock(32, 48, expansion_factor=6, stride=2),# 93, 206\n",
        "            MBConvBlock(48, 64, expansion_factor=6, stride=2),# 47, 103\n",
        "            MBConvBlock(64, 64, expansion_factor=6, stride=1), # 47, 103\n",
        "            MBConvBlock(64, 128, expansion_factor=6, stride=2),# 24, 52\n",
        "            MBConvBlock(128, 128, expansion_factor=6, stride=1),# 24, 52\n",
        "            MBConvBlock(128, 256, expansion_factor=6, stride=2),# 12, 26\n",
        "            MBConvBlock(256, 256, expansion_factor=6, stride=1),# 12, 26\n",
        "            MBConvBlock(256, 512, expansion_factor=6, stride=2), # 6, 13\n",
        "            # MBConvBlock(512, 512, expansion_factor=6, stride=2) # 3, 7\n",
        "            nn.Conv2d(512, 512, kernel_size= 3, stride= 2, groups= 512), #2,6\n",
        "            nn.Conv2d(512, 512, kernel_size= 1, stride= 1) #2,6\n",
        "\n",
        "        )\n",
        "\n",
        "        # # Глобальный Average Pooling\n",
        "        # self.global_pool = nn.AdaptiveAvgPool2d(1)\n",
        "\n",
        "        # Полносвязный слой для классификации\n",
        "        self.fc1 = nn.Linear(512*12, 16)\n",
        "        self.relu_fc = nn.ReLU(inplace=True)\n",
        "        self.dropout_fc = nn.Dropout(p= 0.2) # Dropout после первого FC слоя\n",
        "        self.fc2 = nn.Linear(16, 1)\n",
        "\n",
        "        self._initialize_weights()\n",
        "\n",
        "    def forward(self, image1, image2):\n",
        "        x = torch.cat((image1, image2), dim=1)\n",
        "        x = self.relu1(self.bn1(self.conv_stem(x)))\n",
        "        x = self.blocks(x)\n",
        "        # x = self.relu2(self.bn2(self.conv_head(x)))\n",
        "        # x = self.global_pool(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        # x = self.dropout_fc(self.relu_fc(self.fc1(x)))\n",
        "        x = self.relu_fc(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "    def _initialize_weights(self):\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                nn.init.kaiming_normal_(m.weight, mode='fan_out')\n",
        "                if m.bias is not None:\n",
        "                    nn.init.zeros_(m.bias)\n",
        "            elif isinstance(m, nn.BatchNorm2d):\n",
        "                nn.init.ones_(m.weight)\n",
        "                nn.init.zeros_(m.bias)\n",
        "            elif isinstance(m, nn.Linear):\n",
        "                nn.init.normal_(m.weight, 0, 0.01)\n",
        "                nn.init.zeros_(m.bias)"
      ],
      "metadata": {
        "id": "GLYmp7Yh07xh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train"
      ],
      "metadata": {
        "id": "XS0f1x1sVaqm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time"
      ],
      "metadata": {
        "id": "QSQeLqEEVbx5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, num_epochs, last_epoch, train_loader,\n",
        "                validation_loader,test_loader,optimizer,device,scheduler):\n",
        "    start_time = time.time()\n",
        "    minibatch_loss_list,train_loss_list,valid_loss_list = [] ,[], []\n",
        "    criterion = torch.nn.MSELoss()\n",
        "\n",
        "    for epoch in range(last_epoch, num_epochs):\n",
        "        model.train()\n",
        "        for batch_idx,((feature1,feature2),targets) in enumerate(train_loader):\n",
        "            feature1 = feature1.to(device)\n",
        "            feature2 = feature2.to(device)\n",
        "            targets = targets.to(device)\n",
        "\n",
        "            predicted_y = model(feature1,feature2)\n",
        "            loss = criterion(predicted_y, targets)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            minibatch_loss_list.append(loss.item())\n",
        "            if batch_idx %100 == 0:\n",
        "                print(f'Epoch: {epoch+1:03d}/{num_epochs:03d} '\n",
        "                      f'| Batch {batch_idx:04d}/{len(train_loader):04d} '\n",
        "                      f'| Loss: {loss:.4f}'\n",
        "                      f'| Learning rate: {optimizer.param_groups[0][\"lr\"]}')\n",
        "        scheduler.step()\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            avg_loss_train, accuracy_train = compute_accuracy(model, train_loader, device)\n",
        "            avg_loss_val, accuracy_val = compute_accuracy(model, validation_loader, device)\n",
        "            # scheduler.step( avg_loss_val)\n",
        "            print(f'Epoch: {epoch+1:03d}/{num_epochs:03d} '\n",
        "                  f'| Train AVG LOSS: {avg_loss_train: .4f} | Accuracy_train: {accuracy_train: .4f} \\n'\n",
        "                  f'| Validation AVG LOSS: {avg_loss_val: .4f} | Accuracy_val: {accuracy_val: .4f} ')\n",
        "            train_loss_list.append(avg_loss_train)\n",
        "            valid_loss_list.append(avg_loss_val)\n",
        "\n",
        "        # checkpoint = {\n",
        "        #     'model_state_dict': model.state_dict(),\n",
        "        #     'optimizer_state_dict': optimizer.state_dict()\n",
        "        # }\n",
        "\n",
        "        checkpoint_dict['state_model'] = model.state_dict()\n",
        "        checkpoint_dict['state_opt'] =  optimizer.state_dict()\n",
        "        checkpoint_dict[ 'state_scheduler'] = scheduler.state_dict()\n",
        "        checkpoint_dict['train_loss'] = avg_loss_train\n",
        "        checkpoint_dict['val_loss'] = avg_loss_val\n",
        "        checkpoint_dict['train_acc'] = accuracy_train\n",
        "        checkpoint_dict['val_acc'] =  accuracy_val\n",
        "        checkpoint_dict['EPOCHS'] = num_epochs\n",
        "        checkpoint_dict['current_epoch'] = epoch + 1\n",
        "        checkpoint_dict[ 'learning'] = optimizer.param_groups[0][\"lr\"]\n",
        "\n",
        "        path = directory_path + str(epoch + 1) + '_epoch.pth'\n",
        "        torch.save(checkpoint_dict, path)\n",
        "        # path = '/content/drive/My Drive/checkpoint_model_3.pth'\n",
        "        # torch.save(checkpoint, path)\n",
        "\n",
        "        elapsed = (time.time() - start_time)/60\n",
        "        print(f'Time elapsed: {elapsed:.2f} min')\n",
        "\n",
        "    elapsed = (time.time() - start_time)/60\n",
        "    print(f'Total Training Time: {elapsed:.2f} min')\n",
        "\n",
        "    avg_loss_test, accuracy_test = compute_accuracy(model, test_loader,device)\n",
        "    print(f'Test AVG LOSS: {avg_loss_test: .4f} | Accuracy_test: {accuracy_test: .4f}')\n",
        "\n",
        "    return minibatch_loss_list, train_loss_list, valid_loss_list\n"
      ],
      "metadata": {
        "id": "DCXyjwh3VeBv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "MAIN"
      ],
      "metadata": {
        "id": "I0Rl_zUYg-_e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 8\n",
        "RANDOM_SEED = 123\n",
        "NUM_EPOCHS = 20\n",
        "last_epoch = 0\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"Використовується пристрій: {DEVICE}\")"
      ],
      "metadata": {
        "id": "npJQoHwqg_RN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c80b4362-0525-4c20-c135-c2f1d562c2b5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Використовується пристрій: cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.optim.lr_scheduler import StepLR\n",
        "set_all_seeds(RANDOM_SEED)\n",
        "\n",
        "model = Network()\n",
        "model = model.to(DEVICE)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr= 0.001, weight_decay= 0.01)\n",
        "train_loader, validation_loader, test_loader = get_data(batch_size = BATCH_SIZE)\n",
        "scheduler = StepLR(optimizer, step_size= 4, gamma= 0.3)\n",
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "print(f'The model has {count_parameters(model):,} trainable parameters')\n"
      ],
      "metadata": {
        "id": "b_nYsP82hGE_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8bfb2062-5fb9-46cf-fa84-b1999a689cbf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The model has 3,090,353 trainable parameters\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Загрузка словаря из файла\n",
        "path = './1_epoch.pth'\n",
        "checkpoint = torch.load(path)\n",
        "\n",
        "# # Загрузка состояния модели\n",
        "# save_check = checkpoint['optimizer_state_dict']\n",
        "# save_check['lr'] = 0.0003\n",
        "model.load_state_dict( checkpoint['state_model'] )\n",
        "optimizer.load_state_dict( checkpoint['state_opt'] )\n",
        "scheduler.load_state_dict( checkpoint['state_scheduler'] )\n",
        "last_epoch = checkpoint['current_epoch']\n",
        "# # Загрузка состояния оптимизатора\n",
        "# optimizer.load_state_dict(save_check)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bN_ikLOuoiFC",
        "outputId": "44d73467-afc4-476a-ae20-009855851fba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "minibatch_loss, train_acc, valid_acc = train(model = model,\n",
        "            num_epochs = NUM_EPOCHS,\n",
        "            last_epoch = last_epoch,\n",
        "            train_loader = train_loader,\n",
        "            validation_loader = validation_loader,\n",
        "            test_loader = test_loader,\n",
        "            optimizer = optimizer,\n",
        "            device = DEVICE,\n",
        "            scheduler = scheduler)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PFhblozdpIeB",
        "outputId": "43d0b2c0-64ae-4ec6-839e-e7785ced64fb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 001/020 | Batch 0000/2550 | Loss: 1.3277| Learning rate: 0.001\n",
            "Epoch: 001/020 | Batch 0100/2550 | Loss: 0.0819| Learning rate: 0.001\n",
            "Epoch: 001/020 | Batch 0200/2550 | Loss: 0.0481| Learning rate: 0.001\n",
            "Epoch: 001/020 | Batch 0300/2550 | Loss: 0.1593| Learning rate: 0.001\n",
            "Epoch: 001/020 | Batch 0400/2550 | Loss: 0.2007| Learning rate: 0.001\n",
            "Epoch: 001/020 | Batch 0500/2550 | Loss: 0.1744| Learning rate: 0.001\n",
            "Epoch: 001/020 | Batch 0600/2550 | Loss: 0.3498| Learning rate: 0.001\n",
            "Epoch: 001/020 | Batch 0700/2550 | Loss: 0.1204| Learning rate: 0.001\n",
            "Epoch: 001/020 | Batch 0800/2550 | Loss: 0.2920| Learning rate: 0.001\n",
            "Epoch: 001/020 | Batch 0900/2550 | Loss: 0.0329| Learning rate: 0.001\n",
            "Epoch: 001/020 | Batch 1000/2550 | Loss: 0.2238| Learning rate: 0.001\n",
            "Epoch: 001/020 | Batch 1100/2550 | Loss: 0.1540| Learning rate: 0.001\n",
            "Epoch: 001/020 | Batch 1200/2550 | Loss: 0.0760| Learning rate: 0.001\n",
            "Epoch: 001/020 | Batch 1300/2550 | Loss: 0.1441| Learning rate: 0.001\n",
            "Epoch: 001/020 | Batch 1400/2550 | Loss: 0.9409| Learning rate: 0.001\n",
            "Epoch: 001/020 | Batch 1500/2550 | Loss: 0.1327| Learning rate: 0.001\n",
            "Epoch: 001/020 | Batch 1600/2550 | Loss: 0.0425| Learning rate: 0.001\n",
            "Epoch: 001/020 | Batch 1700/2550 | Loss: 0.1056| Learning rate: 0.001\n",
            "Epoch: 001/020 | Batch 1800/2550 | Loss: 0.0829| Learning rate: 0.001\n",
            "Epoch: 001/020 | Batch 1900/2550 | Loss: 0.1085| Learning rate: 0.001\n",
            "Epoch: 001/020 | Batch 2000/2550 | Loss: 0.1597| Learning rate: 0.001\n",
            "Epoch: 001/020 | Batch 2100/2550 | Loss: 0.1324| Learning rate: 0.001\n",
            "Epoch: 001/020 | Batch 2200/2550 | Loss: 0.1789| Learning rate: 0.001\n",
            "Epoch: 001/020 | Batch 2300/2550 | Loss: 0.0659| Learning rate: 0.001\n",
            "Epoch: 001/020 | Batch 2400/2550 | Loss: 0.3275| Learning rate: 0.001\n",
            "Epoch: 001/020 | Batch 2500/2550 | Loss: 0.3627| Learning rate: 0.001\n",
            "Epoch: 001/020 | Train AVG LOSS:  0.2015 | Accuracy_train:  0.2357 \n",
            "| Validation AVG LOSS:  0.0751 | Accuracy_val:  0.3352 \n",
            "Time elapsed: 18.81 min\n",
            "Epoch: 002/020 | Batch 0000/2550 | Loss: 0.0404| Learning rate: 0.001\n",
            "Epoch: 002/020 | Batch 0100/2550 | Loss: 0.2080| Learning rate: 0.001\n",
            "Epoch: 002/020 | Batch 0200/2550 | Loss: 0.0717| Learning rate: 0.001\n",
            "Epoch: 002/020 | Batch 0300/2550 | Loss: 0.8112| Learning rate: 0.001\n",
            "Epoch: 002/020 | Batch 0400/2550 | Loss: 0.0944| Learning rate: 0.001\n",
            "Epoch: 002/020 | Batch 0500/2550 | Loss: 0.0564| Learning rate: 0.001\n",
            "Epoch: 002/020 | Batch 0600/2550 | Loss: 0.0450| Learning rate: 0.001\n",
            "Epoch: 002/020 | Batch 0700/2550 | Loss: 0.1613| Learning rate: 0.001\n",
            "Epoch: 002/020 | Batch 0800/2550 | Loss: 0.4353| Learning rate: 0.001\n",
            "Epoch: 002/020 | Batch 0900/2550 | Loss: 0.2275| Learning rate: 0.001\n",
            "Epoch: 002/020 | Batch 1000/2550 | Loss: 0.4784| Learning rate: 0.001\n",
            "Epoch: 002/020 | Batch 1100/2550 | Loss: 0.1351| Learning rate: 0.001\n",
            "Epoch: 002/020 | Batch 1200/2550 | Loss: 0.0505| Learning rate: 0.001\n",
            "Epoch: 002/020 | Batch 1300/2550 | Loss: 0.6427| Learning rate: 0.001\n",
            "Epoch: 002/020 | Batch 1400/2550 | Loss: 0.5184| Learning rate: 0.001\n",
            "Epoch: 002/020 | Batch 1500/2550 | Loss: 0.0982| Learning rate: 0.001\n",
            "Epoch: 002/020 | Batch 1600/2550 | Loss: 0.0570| Learning rate: 0.001\n",
            "Epoch: 002/020 | Batch 1700/2550 | Loss: 0.1467| Learning rate: 0.001\n",
            "Epoch: 002/020 | Batch 1800/2550 | Loss: 0.0548| Learning rate: 0.001\n",
            "Epoch: 002/020 | Batch 1900/2550 | Loss: 0.0524| Learning rate: 0.001\n",
            "Epoch: 002/020 | Batch 2000/2550 | Loss: 0.2366| Learning rate: 0.001\n",
            "Epoch: 002/020 | Batch 2100/2550 | Loss: 0.0594| Learning rate: 0.001\n",
            "Epoch: 002/020 | Batch 2200/2550 | Loss: 0.8808| Learning rate: 0.001\n",
            "Epoch: 002/020 | Batch 2300/2550 | Loss: 0.0537| Learning rate: 0.001\n",
            "Epoch: 002/020 | Batch 2400/2550 | Loss: 0.1399| Learning rate: 0.001\n",
            "Epoch: 002/020 | Batch 2500/2550 | Loss: 0.1210| Learning rate: 0.001\n",
            "Epoch: 002/020 | Train AVG LOSS:  0.2005 | Accuracy_train:  0.2294 \n",
            "| Validation AVG LOSS:  0.0814 | Accuracy_val:  0.2931 \n",
            "Time elapsed: 37.35 min\n",
            "Epoch: 003/020 | Batch 0000/2550 | Loss: 0.0548| Learning rate: 0.001\n",
            "Epoch: 003/020 | Batch 0100/2550 | Loss: 0.1612| Learning rate: 0.001\n",
            "Epoch: 003/020 | Batch 0200/2550 | Loss: 0.4766| Learning rate: 0.001\n",
            "Epoch: 003/020 | Batch 0300/2550 | Loss: 0.1178| Learning rate: 0.001\n",
            "Epoch: 003/020 | Batch 0400/2550 | Loss: 0.4124| Learning rate: 0.001\n",
            "Epoch: 003/020 | Batch 0500/2550 | Loss: 0.2294| Learning rate: 0.001\n",
            "Epoch: 003/020 | Batch 0600/2550 | Loss: 0.4858| Learning rate: 0.001\n",
            "Epoch: 003/020 | Batch 0700/2550 | Loss: 0.1418| Learning rate: 0.001\n",
            "Epoch: 003/020 | Batch 0800/2550 | Loss: 0.4081| Learning rate: 0.001\n",
            "Epoch: 003/020 | Batch 0900/2550 | Loss: 0.1827| Learning rate: 0.001\n",
            "Epoch: 003/020 | Batch 1000/2550 | Loss: 0.0617| Learning rate: 0.001\n",
            "Epoch: 003/020 | Batch 1100/2550 | Loss: 0.3587| Learning rate: 0.001\n",
            "Epoch: 003/020 | Batch 1200/2550 | Loss: 0.0698| Learning rate: 0.001\n",
            "Epoch: 003/020 | Batch 1300/2550 | Loss: 0.0403| Learning rate: 0.001\n",
            "Epoch: 003/020 | Batch 1400/2550 | Loss: 0.0634| Learning rate: 0.001\n",
            "Epoch: 003/020 | Batch 1500/2550 | Loss: 0.0878| Learning rate: 0.001\n",
            "Epoch: 003/020 | Batch 1600/2550 | Loss: 0.7369| Learning rate: 0.001\n",
            "Epoch: 003/020 | Batch 1700/2550 | Loss: 0.2902| Learning rate: 0.001\n",
            "Epoch: 003/020 | Batch 1800/2550 | Loss: 0.1732| Learning rate: 0.001\n",
            "Epoch: 003/020 | Batch 1900/2550 | Loss: 0.4463| Learning rate: 0.001\n",
            "Epoch: 003/020 | Batch 2000/2550 | Loss: 0.4892| Learning rate: 0.001\n",
            "Epoch: 003/020 | Batch 2100/2550 | Loss: 0.1251| Learning rate: 0.001\n",
            "Epoch: 003/020 | Batch 2200/2550 | Loss: 0.1236| Learning rate: 0.001\n",
            "Epoch: 003/020 | Batch 2300/2550 | Loss: 0.1881| Learning rate: 0.001\n",
            "Epoch: 003/020 | Batch 2400/2550 | Loss: 0.1423| Learning rate: 0.001\n",
            "Epoch: 003/020 | Batch 2500/2550 | Loss: 0.1023| Learning rate: 0.001\n",
            "Epoch: 003/020 | Train AVG LOSS:  0.2007 | Accuracy_train:  0.2302 \n",
            "| Validation AVG LOSS:  0.0845 | Accuracy_val:  0.2824 \n",
            "Time elapsed: 55.78 min\n",
            "Epoch: 004/020 | Batch 0000/2550 | Loss: 0.1006| Learning rate: 0.001\n",
            "Epoch: 004/020 | Batch 0100/2550 | Loss: 0.0710| Learning rate: 0.001\n",
            "Epoch: 004/020 | Batch 0200/2550 | Loss: 0.1576| Learning rate: 0.001\n",
            "Epoch: 004/020 | Batch 0300/2550 | Loss: 0.1784| Learning rate: 0.001\n",
            "Epoch: 004/020 | Batch 0400/2550 | Loss: 0.1628| Learning rate: 0.001\n",
            "Epoch: 004/020 | Batch 0500/2550 | Loss: 0.0750| Learning rate: 0.001\n",
            "Epoch: 004/020 | Batch 0600/2550 | Loss: 0.1571| Learning rate: 0.001\n",
            "Epoch: 004/020 | Batch 0700/2550 | Loss: 0.0873| Learning rate: 0.001\n",
            "Epoch: 004/020 | Batch 0800/2550 | Loss: 0.2584| Learning rate: 0.001\n",
            "Epoch: 004/020 | Batch 0900/2550 | Loss: 0.1192| Learning rate: 0.001\n",
            "Epoch: 004/020 | Batch 1000/2550 | Loss: 0.0425| Learning rate: 0.001\n",
            "Epoch: 004/020 | Batch 1100/2550 | Loss: 0.1042| Learning rate: 0.001\n",
            "Epoch: 004/020 | Batch 1200/2550 | Loss: 0.1231| Learning rate: 0.001\n",
            "Epoch: 004/020 | Batch 1300/2550 | Loss: 0.3608| Learning rate: 0.001\n",
            "Epoch: 004/020 | Batch 1400/2550 | Loss: 0.0428| Learning rate: 0.001\n",
            "Epoch: 004/020 | Batch 1500/2550 | Loss: 0.0869| Learning rate: 0.001\n",
            "Epoch: 004/020 | Batch 1600/2550 | Loss: 0.1659| Learning rate: 0.001\n",
            "Epoch: 004/020 | Batch 1700/2550 | Loss: 0.1210| Learning rate: 0.001\n",
            "Epoch: 004/020 | Batch 1800/2550 | Loss: 0.5145| Learning rate: 0.001\n",
            "Epoch: 004/020 | Batch 1900/2550 | Loss: 0.1700| Learning rate: 0.001\n",
            "Epoch: 004/020 | Batch 2000/2550 | Loss: 0.4344| Learning rate: 0.001\n",
            "Epoch: 004/020 | Batch 2100/2550 | Loss: 0.0559| Learning rate: 0.001\n",
            "Epoch: 004/020 | Batch 2200/2550 | Loss: 0.1643| Learning rate: 0.001\n",
            "Epoch: 004/020 | Batch 2300/2550 | Loss: 0.5058| Learning rate: 0.001\n",
            "Epoch: 004/020 | Batch 2400/2550 | Loss: 0.0558| Learning rate: 0.001\n",
            "Epoch: 004/020 | Batch 2500/2550 | Loss: 0.1021| Learning rate: 0.001\n",
            "Epoch: 004/020 | Train AVG LOSS:  0.2008 | Accuracy_train:  0.2359 \n",
            "| Validation AVG LOSS:  0.0776 | Accuracy_val:  0.3164 \n",
            "Time elapsed: 74.14 min\n",
            "Epoch: 005/020 | Batch 0000/2550 | Loss: 0.0686| Learning rate: 0.0003\n",
            "Epoch: 005/020 | Batch 0100/2550 | Loss: 0.2010| Learning rate: 0.0003\n",
            "Epoch: 005/020 | Batch 0200/2550 | Loss: 0.1048| Learning rate: 0.0003\n",
            "Epoch: 005/020 | Batch 0300/2550 | Loss: 0.1081| Learning rate: 0.0003\n",
            "Epoch: 005/020 | Batch 0400/2550 | Loss: 0.0838| Learning rate: 0.0003\n",
            "Epoch: 005/020 | Batch 0500/2550 | Loss: 0.7926| Learning rate: 0.0003\n",
            "Epoch: 005/020 | Batch 0600/2550 | Loss: 0.0982| Learning rate: 0.0003\n",
            "Epoch: 005/020 | Batch 0700/2550 | Loss: 0.4336| Learning rate: 0.0003\n",
            "Epoch: 005/020 | Batch 0800/2550 | Loss: 0.0788| Learning rate: 0.0003\n",
            "Epoch: 005/020 | Batch 0900/2550 | Loss: 0.0444| Learning rate: 0.0003\n",
            "Epoch: 005/020 | Batch 1000/2550 | Loss: 0.0566| Learning rate: 0.0003\n",
            "Epoch: 005/020 | Batch 1100/2550 | Loss: 0.3809| Learning rate: 0.0003\n",
            "Epoch: 005/020 | Batch 1200/2550 | Loss: 0.3877| Learning rate: 0.0003\n",
            "Epoch: 005/020 | Batch 1300/2550 | Loss: 0.2650| Learning rate: 0.0003\n",
            "Epoch: 005/020 | Batch 1400/2550 | Loss: 0.1000| Learning rate: 0.0003\n",
            "Epoch: 005/020 | Batch 1500/2550 | Loss: 0.0518| Learning rate: 0.0003\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "v_SsiXR0y7IO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fce3f8be-3dcb-4fff-8b2c-c6e20dd00e6c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# scheduler = StepLR(optimizer, step_size=1, gamma=1)\n",
        "minibatch_loss, train_acc, valid_acc = train(model = model,\n",
        "            num_epochs = 20,\n",
        "            last_epoch= 1,\n",
        "            train_loader = train_loader,\n",
        "            validation_loader = validation_loader,\n",
        "            test_loader = test_loader,\n",
        "            optimizer = optimizer,\n",
        "            device = DEVICE,\n",
        "            scheduler = scheduler)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 411
        },
        "id": "FVILHlVn27CH",
        "outputId": "d285f682-af13-460b-e6dc-8dd385c70f99"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'Network' object has no attribute 'fc1'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-20-debc64df5473>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# scheduler = StepLR(optimizer, step_size=1, gamma=1)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m minibatch_loss, train_acc, valid_acc = train(model = model,\n\u001b[0m\u001b[1;32m      3\u001b[0m             \u001b[0mnum_epochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m             \u001b[0mlast_epoch\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m             \u001b[0mtrain_loader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-12-b02203f5da8d>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, num_epochs, last_epoch, train_loader, validation_loader, test_loader, optimizer, device, scheduler)\u001b[0m\n\u001b[1;32m     12\u001b[0m             \u001b[0mtargets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m             \u001b[0mpredicted_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeature1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfeature2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredicted_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-15-31efd9515a9b>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, image1, image2)\u001b[0m\n\u001b[1;32m     74\u001b[0m         \u001b[0;31m# x = self.global_pool(x)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout_fc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu_fc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1926\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodules\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1927\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mmodules\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1928\u001b[0;31m         raise AttributeError(\n\u001b[0m\u001b[1;32m   1929\u001b[0m             \u001b[0;34mf\"'{type(self).__name__}' object has no attribute '{name}'\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1930\u001b[0m         )\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'Network' object has no attribute 'fc1'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "minibatch_loss, train_acc, valid_acc = train(model = model,\n",
        "            num_epochs = 20,\n",
        "            last_epoch= 2,\n",
        "            train_loader = train_loader,\n",
        "            validation_loader = validation_loader,\n",
        "            test_loader = test_loader,\n",
        "            optimizer = optimizer,\n",
        "            device = DEVICE,\n",
        "            scheduler = scheduler)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 931
        },
        "id": "BEjLc_wtnXvD",
        "outputId": "f343e69e-7373-4265-8041-0fb532d3f0ff"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 003/020 | Batch 0000/2550 | Loss: 0.0806| Learning rate: 0.0003\n",
            "Epoch: 003/020 | Batch 0100/2550 | Loss: 0.0788| Learning rate: 0.0003\n",
            "Epoch: 003/020 | Batch 0200/2550 | Loss: 0.0338| Learning rate: 0.0003\n",
            "Epoch: 003/020 | Batch 0300/2550 | Loss: 0.1559| Learning rate: 0.0003\n",
            "Epoch: 003/020 | Batch 0400/2550 | Loss: 0.0443| Learning rate: 0.0003\n",
            "Epoch: 003/020 | Batch 0500/2550 | Loss: 0.1884| Learning rate: 0.0003\n",
            "Epoch: 003/020 | Batch 0600/2550 | Loss: 0.0161| Learning rate: 0.0003\n",
            "Epoch: 003/020 | Batch 0700/2550 | Loss: 0.0415| Learning rate: 0.0003\n",
            "Epoch: 003/020 | Batch 0800/2550 | Loss: 0.0288| Learning rate: 0.0003\n",
            "Epoch: 003/020 | Batch 0900/2550 | Loss: 0.0450| Learning rate: 0.0003\n",
            "Epoch: 003/020 | Batch 1000/2550 | Loss: 0.0454| Learning rate: 0.0003\n",
            "Epoch: 003/020 | Batch 1100/2550 | Loss: 0.0244| Learning rate: 0.0003\n",
            "Epoch: 003/020 | Batch 1200/2550 | Loss: 0.0311| Learning rate: 0.0003\n",
            "Epoch: 003/020 | Batch 1300/2550 | Loss: 0.0623| Learning rate: 0.0003\n",
            "Epoch: 003/020 | Batch 1400/2550 | Loss: 0.0885| Learning rate: 0.0003\n",
            "Epoch: 003/020 | Batch 1500/2550 | Loss: 0.0790| Learning rate: 0.0003\n",
            "Epoch: 003/020 | Batch 1600/2550 | Loss: 0.0727| Learning rate: 0.0003\n",
            "Epoch: 003/020 | Batch 1700/2550 | Loss: 0.1049| Learning rate: 0.0003\n",
            "Epoch: 003/020 | Batch 1800/2550 | Loss: 0.0460| Learning rate: 0.0003\n",
            "Epoch: 003/020 | Batch 1900/2550 | Loss: 0.0755| Learning rate: 0.0003\n",
            "Epoch: 003/020 | Batch 2000/2550 | Loss: 0.0214| Learning rate: 0.0003\n",
            "Epoch: 003/020 | Batch 2100/2550 | Loss: 0.0133| Learning rate: 0.0003\n",
            "Epoch: 003/020 | Batch 2200/2550 | Loss: 0.1110| Learning rate: 0.0003\n",
            "Epoch: 003/020 | Batch 2300/2550 | Loss: 0.0380| Learning rate: 0.0003\n",
            "Epoch: 003/020 | Batch 2400/2550 | Loss: 0.0422| Learning rate: 0.0003\n",
            "Epoch: 003/020 | Batch 2500/2550 | Loss: 0.1103| Learning rate: 0.0003\n",
            "Epoch: 003/020 | Train AVG LOSS:  0.0334 | Accuracy_train:  0.3855 \n",
            "| Validation AVG LOSS:  0.0697 | Accuracy_val:  0.2836 \n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "Parent directory /content/drive/My Drive/model_3 does not exist.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-22-7eb54c476544>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m minibatch_loss, train_acc, valid_acc = train(model = model,\n\u001b[0m\u001b[1;32m      2\u001b[0m             \u001b[0mnum_epochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m             \u001b[0mlast_epoch\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m             \u001b[0mtrain_loader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m             \u001b[0mvalidation_loader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalidation_loader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-11-f25870ccec88>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, num_epochs, last_epoch, train_loader, validation_loader, test_loader, optimizer, device, scheduler)\u001b[0m\n\u001b[1;32m     43\u001b[0m         }\n\u001b[1;32m     44\u001b[0m         \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'/content/drive/My Drive/model_3/'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'_epoch.pth'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheckpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m         \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'/content/drive/My Drive/checkpoint_model_3.pth'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheckpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization, _disable_byteorder_record)\u001b[0m\n\u001b[1;32m    941\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    942\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_use_new_zipfile_serialization\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 943\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0m_open_zipfile_writer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mopened_zipfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    944\u001b[0m             _save(\n\u001b[1;32m    945\u001b[0m                 \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_open_zipfile_writer\u001b[0;34m(name_or_buffer)\u001b[0m\n\u001b[1;32m    808\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    809\u001b[0m         \u001b[0mcontainer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_open_zipfile_writer_buffer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 810\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mcontainer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    811\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    812\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    779\u001b[0m             )\n\u001b[1;32m    780\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 781\u001b[0;31m             \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPyTorchFileWriter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_compute_crc32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    782\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    783\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__exit__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Parent directory /content/drive/My Drive/model_3 does not exist."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "minibatch_loss, train_acc, valid_acc = train(model = model,\n",
        "            num_epochs = 20,\n",
        "            last_epoch= 3,\n",
        "            train_loader = train_loader,\n",
        "            validation_loader = validation_loader,\n",
        "            test_loader = test_loader,\n",
        "            optimizer = optimizer,\n",
        "            device = DEVICE,\n",
        "            scheduler = scheduler)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 931
        },
        "id": "Mtd1r-4EkH4p",
        "outputId": "35e7e751-1fa7-4ce0-fa4f-f5030d07ce04"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 004/020 | Batch 0000/2550 | Loss: 0.0347| Learning rate: 0.0003\n",
            "Epoch: 004/020 | Batch 0100/2550 | Loss: 0.0388| Learning rate: 0.0003\n",
            "Epoch: 004/020 | Batch 0200/2550 | Loss: 0.0183| Learning rate: 0.0003\n",
            "Epoch: 004/020 | Batch 0300/2550 | Loss: 0.0252| Learning rate: 0.0003\n",
            "Epoch: 004/020 | Batch 0400/2550 | Loss: 0.0619| Learning rate: 0.0003\n",
            "Epoch: 004/020 | Batch 0500/2550 | Loss: 0.0178| Learning rate: 0.0003\n",
            "Epoch: 004/020 | Batch 0600/2550 | Loss: 0.0264| Learning rate: 0.0003\n",
            "Epoch: 004/020 | Batch 0700/2550 | Loss: 0.0275| Learning rate: 0.0003\n",
            "Epoch: 004/020 | Batch 0800/2550 | Loss: 0.0295| Learning rate: 0.0003\n",
            "Epoch: 004/020 | Batch 0900/2550 | Loss: 0.0397| Learning rate: 0.0003\n",
            "Epoch: 004/020 | Batch 1000/2550 | Loss: 0.0522| Learning rate: 0.0003\n",
            "Epoch: 004/020 | Batch 1100/2550 | Loss: 0.0464| Learning rate: 0.0003\n",
            "Epoch: 004/020 | Batch 1200/2550 | Loss: 0.0921| Learning rate: 0.0003\n",
            "Epoch: 004/020 | Batch 1300/2550 | Loss: 0.0252| Learning rate: 0.0003\n",
            "Epoch: 004/020 | Batch 1400/2550 | Loss: 0.1777| Learning rate: 0.0003\n",
            "Epoch: 004/020 | Batch 1500/2550 | Loss: 0.0802| Learning rate: 0.0003\n",
            "Epoch: 004/020 | Batch 1600/2550 | Loss: 0.0513| Learning rate: 0.0003\n",
            "Epoch: 004/020 | Batch 1700/2550 | Loss: 0.0451| Learning rate: 0.0003\n",
            "Epoch: 004/020 | Batch 1800/2550 | Loss: 0.0742| Learning rate: 0.0003\n",
            "Epoch: 004/020 | Batch 1900/2550 | Loss: 0.0395| Learning rate: 0.0003\n",
            "Epoch: 004/020 | Batch 2000/2550 | Loss: 0.0736| Learning rate: 0.0003\n",
            "Epoch: 004/020 | Batch 2100/2550 | Loss: 0.0280| Learning rate: 0.0003\n",
            "Epoch: 004/020 | Batch 2200/2550 | Loss: 0.0176| Learning rate: 0.0003\n",
            "Epoch: 004/020 | Batch 2300/2550 | Loss: 0.0476| Learning rate: 0.0003\n",
            "Epoch: 004/020 | Batch 2400/2550 | Loss: 0.0484| Learning rate: 0.0003\n",
            "Epoch: 004/020 | Batch 2500/2550 | Loss: 0.0182| Learning rate: 0.0003\n",
            "Epoch: 004/020 | Train AVG LOSS:  0.0764 | Accuracy_train:  0.1940 \n",
            "| Validation AVG LOSS:  0.1176 | Accuracy_val:  0.1616 \n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "Parent directory /content/drive/My Drive/model_3 does not exist.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-25-469ceb616a98>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m minibatch_loss, train_acc, valid_acc = train(model = model,\n\u001b[0m\u001b[1;32m      2\u001b[0m             \u001b[0mnum_epochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m             \u001b[0mlast_epoch\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m             \u001b[0mtrain_loader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m             \u001b[0mvalidation_loader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalidation_loader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-11-f25870ccec88>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, num_epochs, last_epoch, train_loader, validation_loader, test_loader, optimizer, device, scheduler)\u001b[0m\n\u001b[1;32m     43\u001b[0m         }\n\u001b[1;32m     44\u001b[0m         \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'/content/drive/My Drive/model_3/'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'_epoch.pth'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheckpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m         \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'/content/drive/My Drive/checkpoint_model_3.pth'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheckpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization, _disable_byteorder_record)\u001b[0m\n\u001b[1;32m    941\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    942\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_use_new_zipfile_serialization\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 943\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0m_open_zipfile_writer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mopened_zipfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    944\u001b[0m             _save(\n\u001b[1;32m    945\u001b[0m                 \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_open_zipfile_writer\u001b[0;34m(name_or_buffer)\u001b[0m\n\u001b[1;32m    808\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    809\u001b[0m         \u001b[0mcontainer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_open_zipfile_writer_buffer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 810\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mcontainer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    811\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    812\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    779\u001b[0m             )\n\u001b[1;32m    780\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 781\u001b[0;31m             \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPyTorchFileWriter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_compute_crc32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    782\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    783\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__exit__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Parent directory /content/drive/My Drive/model_3 does not exist."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "criterion1 = torch.nn.MSELoss()\n",
        "def test_single_example(model, data_loader, device):\n",
        "    # Переводим модель в режим оценки\n",
        "    model.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        # Извлекаем один батч из data_loader\n",
        "        for (feature1, feature2), targets in data_loader:\n",
        "            # Переводим данные на нужное устройство (GPU/CPU)\n",
        "            feature1 = feature1.to(device)\n",
        "            feature2 = feature2.to(device)\n",
        "            targets = targets.to(device)\n",
        "\n",
        "            # Возьмем только первый пример из батча (индекс 0)\n",
        "            feature1_single = feature1[0].unsqueeze(0)  # Убираем размерность батча (берем первый элемент)\n",
        "            feature2_single = feature2[0].unsqueeze(0)  # Убираем размерность батча (берем первый элемент)\n",
        "            target_single = targets[0].unsqueeze(0)    # Убираем размерность батча (берем первый элемент)\n",
        "\n",
        "            # Пропускаем один пример через модель\n",
        "            predicted_y = model(feature1_single, feature2_single)\n",
        "            loss = criterion1(predicted_y, targets)\n",
        "            print(\"loss: \",loss)\n",
        "            # Печатаем предсказанные и настоящие значения\n",
        "            print(f\"Predicted values: {predicted_y}\")\n",
        "            print(f\"Actual values: {target_single}\")\n",
        "\n",
        "            # Можно вывести разницу между предсказанными и настоящими значениями\n",
        "            difference = predicted_y - target_single\n",
        "            print(f\"Difference: {difference}\")\n",
        "\n",
        "            break  # Останавливаем цикл после первого батча\n",
        "\n",
        "# Пример использования функции\n",
        "# Допустим, у вас уже есть model, data_loader и device\n",
        "test_single_example(model, validation_loader, DEVICE)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g2U838D08E6e",
        "outputId": "40ab5969-4b0d-49e7-e077-b9981659bad9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([8, 12])) that is different to the input size (torch.Size([1, 12])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss:  tensor(0.0022, device='cuda:0')\n",
            "Predicted values: tensor([[ 0.9607, -0.0090,  0.0123, -0.0979, -0.0094,  0.9581, -0.0131, -0.0852,\n",
            "         -0.0140, -0.0086,  0.9567,  0.2936]], device='cuda:0')\n",
            "Actual values: tensor([[ 0.9999, -0.0031,  0.0117,  0.0214,  0.0031,  1.0000,  0.0011, -0.0085,\n",
            "         -0.0117, -0.0011,  0.9999,  0.2881]], device='cuda:0')\n",
            "Difference: tensor([[-0.0392, -0.0060,  0.0006, -0.1193, -0.0125, -0.0419, -0.0142, -0.0767,\n",
            "         -0.0023, -0.0075, -0.0433,  0.0055]], device='cuda:0')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "checkpoint = {\n",
        "    'model_state_dict': model.state_dict(),\n",
        "    'optimizer_state_dict': optimizer.state_dict()\n",
        "}\n",
        "path = '/content/drive/My Drive/checkpoint_model3_4.pth'\n",
        "torch.save(checkpoint, path)"
      ],
      "metadata": {
        "id": "ImMMuZkRA30A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "path = '/content/drive/My Drive/checkpoint_model3_4.pth'\n",
        "# Загрузка словаря из файла\n",
        "checkpoint = torch.load(path)\n",
        "\n",
        "# Загрузка состояния модели\n",
        "model.load_state_dict(checkpoint['model_state_dict'])\n",
        "\n",
        "# Загрузка состояния оптимизатора\n",
        "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n"
      ],
      "metadata": {
        "id": "VHmvLTJFBTO_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "current_lr_after_scheduler = optimizer.param_groups[0]['lr']\n",
        "print(current_lr_after_scheduler)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VZ0uRfjYogiu",
        "outputId": "c2b57e54-479b-4210-8866-4406f87d8717"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3e-06\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "print(f'The model has {count_parameters(model):,} trainable parameters')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v64Z6q5iK3MA",
        "outputId": "5bfc96ba-b695-49db-ffa0-893a66275dfe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The model has 975,249 trainable parameters\n"
          ]
        }
      ]
    }
  ]
}